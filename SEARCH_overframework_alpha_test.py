# This script is used to analyze the output of the SEARCH test framework for alpha sensitivity analysis
# The script reads the log files generated by the SEARCH test framework and server pcap file and extracts the following information:
# 1. Exit time
# 2. Percentage of exit before loss
# 3. Average delivery rate
# 4. Missed bins over time
# 5. Throughput over time
# 6. RTT analysis
# 7. CDF of missed bins
# 8. CDF of missed bins before exit
# 9. CDF of delivery rate
# 10. CDF of avg throughput
# 11. CDF of throughput at time of loss
# 12. CDF of time of first loss
# 13. Missed bins vs initial RTT
# 14. Average exit time


import matplotlib.pyplot as plt
from matplotlib.colors import Normalize
import numpy as np
import re
import pandas as pd
import os
import bisect
from scipy.stats import rankdata
from scipy.interpolate import interp1d

SEARCH_RESULT = True
ANALYSIS_PCAP_FILE = True
ANALYSIS_LOG_CSV = True

cwd = os.getcwd()
base_path = os.path.join(cwd, "/home/maryam/SEARCH/Linux/search_test/all_data/test_over_different_link_for_early_exit_problem/fios_cable/data/data/testframework_output_fios_cable2/diff_extra_bins/extra_bin_num21")
pcap_csv_path = os.path.join(cwd, "Linux/search_test/all_data/test_over_different_link_for_early_exit_problem/fios_cable/data/data/pcap_server")
log_csv_path = os.path.join(cwd, "Linux/search_test/all_data/test_over_different_link_for_early_exit_problem/fios_cable/data/data/log_cubic/")
fig_path = os.path.join(cwd, "Linux/search_test/all_data/test_over_different_link_for_early_exit_problem/fios_cable/figs_for_diff_extra_bin_num/extra_bin_num21")
os.makedirs(fig_path, exist_ok=True)

config_output = None

total_bins = 31 

############################### Functions #########################################
def get_data(file_path):
    with open(file_path, 'r') as f:
        lines = f.readlines()

    now_us = None
    now_list = []
    loss_happen_list = []
    exit_time_list = []
    rtt_ms_list = []
    bytes_acked_list = []
    passed_bin_list = []
    passed_bin_time_list = []
    bin_end_time = []
    time_bin_update = []
    bin_snapshots = []
    current_bin_values = {}
    bin_index_list = []
    bin_index_reset = []
    all_bin_values = []
    reset_flag = 0
    missed_bin_flag = 0
    bin_updates_counter = 0
    all_bin_values_passed_bin_filled = []
    exit_search_bin_index = 0
    search_do_bin_indices = []
    norm_list =[]
    rtt_in_each_bin_update = {}
    bin_index_no_reset = 0
    time_each_bin_filled = {}
    reset_times = []
    needed_extra_bins_to_do_search = []


    for i, line in enumerate(lines):
        if i == 7853:
            cc = 0
        if "now_us:" in line:
            now_us = int(re.search(r"now_us: (\d+)", line).group(1)) * 1e-6 # Convert to seconds
            now_list.append(now_us)
        elif "loss happen:" in line:
            loss_happen = int(re.search(r"loss happen: (\d+)", line).group(1))
            loss_happen_list.append(loss_happen)
        elif "passed_bin" in line:
            match = re.search(r"passed_bins?\s+(\d+)", line)
            if match:  # Ensure match is not None
                passed_bin = int(match.group(1))
                passed_bin_list.append(passed_bin)
                missed_bin_flag = 1
                # Search for next occurrence of "now_us" after "passed_bin"
                time_match = None
                for j in range(i + 1, len(lines)):  # Start searching from the next line
                    time_match = re.search(r"now_us: (\d+)", lines[j])
                    if time_match:
                        passed_bin_time_list.append(int(time_match.group(1)) * 1e-6)  # Convert to seconds
                        break    
        elif "Exit Slow Start at" in line:
            exit_time = int(re.search(r"Exit Slow Start at (\d+)", line).group(1)) * 1e-6 # Convert to seconds
            exit_time_list.append(exit_time)
            exit_search_bin_index = ((len(all_bin_values))+passed_bin - 1) if len(all_bin_values) > 0 else 0         
        elif "bytes_acked" in line:
            bytes_acked = int(re.search(r"bytes_acked: (\d+)", line).group(1)) * 1e-6 * 8 # Convert to Mb
            bytes_acked_list.append(bytes_acked)
        elif "Bin end time" in line:
            bin_end_time_ = int(re.search(r"Bin end time: (\d+)", line).group(1)) * 1e-6 # Convert to Second
            bin_end_time.append(bin_end_time_)
        elif "rtt_us" in line:
            rtt_ms = int(re.search(r"rtt_us: (\d+)", line).group(1)) * 1e-3 # Convert to ms
            rtt_ms_list.append(rtt_ms) 
        elif "Need " in line:
            match = re.search(r"Need (\d+) extra bins", line)
            if match:
                needed_extra_bins = int(match.group(1))
                needed_extra_bins_to_do_search.append(needed_extra_bins)
        elif "Current bin index" in line:
            bin_index_ = int(re.search(r"Current bin index: (\d+)", line).group(1))
            bin_index_list.append(bin_index_) 
            if len(bin_index_list) == 1:
                rtt_in_each_bin_update[bin_index_] = rtt_ms  
                time_each_bin_filled[bin_index_] = now_us
                bin_index_no_reset = bin_index_
            if len(bin_index_list) > 1 and passed_bin > 1:
                bin_index_no_reset = bin_index_no_reset + passed_bin
            elif len(bin_index_list) > 1 and bin_index_ > bin_index_list[-2]:
                bin_index_no_reset += 1
            # if the last key in the rtt_in_each_bin_update is less than bin_index_no_reset - 1, fill the between values with the last value
            if len(bin_index_list) > 1 and bin_index_no_reset - 1 > max(rtt_in_each_bin_update.keys()):
                for j in range(max(rtt_in_each_bin_update.keys()) + 1, bin_index_no_reset):
                    # set to last value
                    if j not in rtt_in_each_bin_update:
                        rtt_in_each_bin_update[j] = rtt_in_each_bin_update[max(rtt_in_each_bin_update.keys())]
                        time_each_bin_filled[j] = time_each_bin_filled[max(time_each_bin_filled.keys())]

                rtt_in_each_bin_update[bin_index_no_reset] = rtt_ms 
                time_each_bin_filled[bin_index_no_reset] = now_us
            else:
                if bin_index_no_reset > max(rtt_in_each_bin_update.keys()):       
                    rtt_in_each_bin_update[bin_index_no_reset] = rtt_ms 
                    time_each_bin_filled[bin_index_no_reset] = now_us     
        elif "Scale factor" in line:
            scale_factor = int(re.search(r"Scale factor: (\d+)", line).group(1))    
        elif "reset by missed_bins:" in line:
            reset_flag = 1
            time_match = None
            for j in range(i + 1, len(lines)):  # Start searching from the next line
                time_match = re.search(r"now_us: (\d+)", lines[j])
                if time_match:
                    reset_times.append(int(time_match.group(1)) * 1e-6)  # Convert to seconds
                    break  
        elif "norm " in line:
            match = re.search(r"norm (-?\d+(?:\.\d+)?)", line)
            if match:
                norm_value = float(match.group(1))
                if (norm_value < 0 or norm_value > 100):
                    norm_value = 0
            search_do_bin_indices.append((len(all_bin_values) + passed_bin - 1) if len(all_bin_values) > 0 else 0)
            norm_list.append(norm_value)
        if (len(now_list) == 1) or (now_us and len(bin_end_time) >= 2 and now_us >= bin_end_time[-2]):  
            if "Bin[" in line:
                match = re.match(r"    Bin\[(\d+)\]:\s+(\d+)", line)
                if match:
                    bin_index = int(match.group(1))
                    bin_value = int(match.group(2)) * 1e-6 * 8 # Convert to Mb
                    if scale_factor > 0:
                        bin_value *= 2**scale_factor
                    current_bin_values[bin_index] = bin_value   

                if missed_bin_flag == 0:
                    if bin_index == bin_index_ % total_bins and bin_index_ == bin_updates_counter:
                        all_bin_values.append(bin_value)
                        all_bin_values_passed_bin_filled.append(bin_value)
                        bin_updates_counter += 1
                else: 
                    if reset_flag == 0:
                        if bin_index == bin_index_ % total_bins and bin_index_ == bin_updates_counter + passed_bin - 1:
                            missed = 0
                            for m in range(passed_bin - 1):
                                all_bin_values.append(None)
                                missed += 1
                                all_bin_values_passed_bin_filled.append(all_bin_values[-1 - missed])
                                bin_updates_counter += 1
                            all_bin_values.append(bin_value)
                            all_bin_values_passed_bin_filled.append(bin_value)
                            bin_updates_counter += 1
                            missed_bin_flag = 0
                    else:
                        bin_updates_counter = 0
                        bin_index_reset.append((len(all_bin_values) - 1 + passed_bin))

                        for n in range(passed_bin - 1):
                            all_bin_values_passed_bin_filled.append(all_bin_values[-1])

                        for n in range(passed_bin - 1):
                            all_bin_values.append(None)

                        if bin_index == 0:
                            all_bin_values.append(bin_value)
                            all_bin_values_passed_bin_filled.append(bin_value)
                            bin_updates_counter += 1
                        reset_flag = 0
                        missed_bin_flag = 0

        # End of section, store the bin snapshot
        if len(current_bin_values) == total_bins:
            time_bin_update.append(now_us)
            bin_snapshots.append(current_bin_values)
            current_bin_values = {}
            passed_bin = 1

    return now_list, loss_happen_list, exit_time_list, passed_bin_list, passed_bin_time_list, rtt_ms_list, \
           bytes_acked_list, time_bin_update, bin_snapshots, bin_index_list, all_bin_values, bin_index_reset,  \
           all_bin_values_passed_bin_filled, exit_search_bin_index, search_do_bin_indices, norm_list, rtt_in_each_bin_update, \
           time_each_bin_filled, reset_times, bin_end_time, needed_extra_bins_to_do_search


def calculate_delivery_rate_per_ack(bytes_acked, now, rtt):
    delivery_rates = []
    time_cal_delv_rates = []
    start_index = None

    for i in range(len(now)):
        target_time = now[i] - rtt[i]
        if target_time <= 0:
            start_index = i + 1
            continue

        # Find the interval [j, j+1] such that now[j] <= target_time < now[j+1]
        j = bisect.bisect_right(now, target_time, 0, i) - 1
        if j < 0 or j + 1 >= len(now):
            start_index = i + 1
            continue

        # Linear interpolation of bytes_acked at target_time
        t0, t1 = now[j], now[j + 1]
        b0, b1 = bytes_acked[j], bytes_acked[j + 1]

        if t1 == t0:
            # Avoid division by zero
            start_index = i + 1
            continue

        interpolated_bytes = b0 + (b1 - b0) * (target_time - t0) / (t1 - t0)
        delta_bytes = bytes_acked[i] - interpolated_bytes
        rate = delta_bytes / rtt[i]

        delivery_rates.append(rate)
        time_cal_delv_rates.append(now[i])

    if delivery_rates:
        return delivery_rates, start_index, time_cal_delv_rates
    else:
        return [], start_index, []


def extract_number(folder_name):
    try:
        return int(folder_name.replace("alpha", ""))
    except ValueError:
        return float('inf')  # puts invalid entries at the end
    

def calculate_window_of_data (time, window_size, bytes_acked, bytes_sent, rtt):

    delivered_bytes_window = []
    sent_bytes_window = []
    time_window = []
    rtt_related_to_window = []

    for i in range(len(time)):
        end_of_window_idx = np.where(time <= time.iloc[i] + window_size)[0]
        if len(end_of_window_idx) > 0:
            end_of_window_idx = end_of_window_idx[-1]
            delivered_bytes_window.append(bytes_acked.iloc[end_of_window_idx] - bytes_acked.iloc[i])
            sent_bytes_window.append(bytes_sent.iloc[end_of_window_idx] - bytes_sent.iloc[i])
            time_window.append(time.iloc[end_of_window_idx])
            rtt_related_to_window.append(rtt.iloc[end_of_window_idx]) 

        else:
            delivered_bytes_window.append(None)
            sent_bytes_window.append(None)
            time_window.append(None)
            rtt_related_to_window.append(None)

    return delivered_bytes_window, sent_bytes_window, time_window, rtt_related_to_window
###################################### SEARCH ##############################################################
if SEARCH_RESULT:
    # Store results for all alpha folders
    # alpha_sensitivity_exit_time_dict = {}
    # alpha_sensitivity_percentage_exit_before_loss_dict = {}
    avg_avg_delivery_rate_after_exit_before_loss_dict = {}
    avg_avg_rtt_after_exit_before_loss_dict = {}
    avg_delivery_rate_at_exit_over_time_dict = {}
    avg_delivery_rate_at_loss_over_time_dict = {}
    cdf_diff_time_loss_exit_over_rtt_dict = {}
    cdf_diff_time_loss_exit_dict = {}
    cdf_normalized_diff_delivery_rate_loss_exit_dict = {}
    cdf_diff_delivery_rate_loss_exit_dict = {}
    diff_delivery_rate_loss_exit_dict_even_not_exit = {} # used for ranking
    exit_time_dict = {}
    avg_exit_time_dict = {}
    avg_loss_time_dict = {}
    avg_avg_power_dict = {} # calculated per bins
    avg_avg_power_prime_dict = {} # calculated per bins
    avg_avg_throughput_dict = {} # calculated over time
    avg_avg_throughput_until_loss_dict = {} # calculated over time until loss
    avg_throughput_at_exit_dict = {}
    avg_throughput_at_loss_dict = {}
    avg_avg_rtt_dict = {} # calculated over time
    avg_rtt_at_exit_dict = {}
    avg_rtt_at_loss_dict = {}
    avg_delivery_rate_dict = {} # calculated per bins
    avg_headrooms_dict = {}
    avg_successful_exit_dict = {} # successful: if exit happens after power peak and before loss
    avg_successful_exit_based_power_prime_dict = {}
    avg_failure_exit_less_max_dict = {} # failure: if exit happens before power peak and before loss
    avg_failure_not_exit_dict = {} # failure: if exit does not happen before loss
    donot_exit_percentage_dict = {}
    samples_not_exit_dict = {}
    cdf_diff_rtt_loss_exit_dict = {}
    folder_names = []
    avg_max_power_before_exit_dict = {}
    avg_power_at_exit_dict = {}
    avg_power_at_loss_dict = {}
    success_exit_dict = {}
    success_when_ref_failed = {}
    avg_total_missed_bins_dict = {}
    avg_num_of_resets_dict = {}
    # cdf_dif_power_loss_exit_dict = {}

    # power over time
    avg_avg_power_over_time_dict = {}
    avg_avg_power_prime_over_time_dict = {}
    avg_max_power_before_exit_over_time_dict = {}
    avg_max_power_prime_before_exit_over_time_dict = {}
    avg_power_at_exit_over_time_dict = {}
    avg_power_at_loss_over_time_dict = {}
    success_exit_over_time_dict = {}
    success_when_ref_failed_over_time = {}
    success_exit_over_time_based_power_prime_dict = {}
    avg_failure_exit_less_max_over_time_dict = {}
    avg_failure_not_exit_over_time_dict = {}
    avg_success_exit_over_time_dict = {}
    avg_success_exit_over_time_based_power_prime_dict = {}

 

    for folder in sorted(os.listdir(base_path), key=extract_number):
        folder_path = os.path.join(base_path, folder)

        if os.path.isdir(folder_path) and folder.startswith("alpha"):
            print("Processing:", folder)
            folder_names.append(folder)

            # Define the full path for the subdirectory (if applicable)
            if config_output is not None:
                subfolder_path = os.path.join(folder_path, config_output)
            else:
                subfolder_path = folder_path
            #figures_path for subfolder
            fig_subfolder_path = os.path.join(fig_path, folder)
            os.makedirs(fig_subfolder_path, exist_ok=True)

            # Ensure the subfolder exists before accessing it
            if os.path.exists(subfolder_path) and os.path.isdir(subfolder_path):
                # count the number of files in the directory
                num_files = len([name for name in os.listdir(subfolder_path) if name.endswith(".txt")])

            exit_before_loss_files = []
            exit_time_output_list = []
            avg_throughput_list = []
            avg_throughput_until_loss_list = []
            throughput_at_exit_list = []
            throughput_at_loss_list = []
            avg_delivery_rate_after_exit_before_loss_list = []
            avg_rtt_after_exit_before_loss_list = []
            # throughput_at_time_loss = []
            time_of_first_loss_list = []
            avg_rtt_list = []
            initial_rtt_files = []
            max_rtt_files = []
            median_throughput_files = []
            total_missed_bins = []
            max_missed_bins = []
            total_resets_list = []
            delivery_rate_at_exit_list = [] #delivery rate at exit
            delivery_rate_At_loss_list = [] #delivery rate at loss
            delivery_rate_exit_over_time_list = []
            delivery_rate_loss_over_time_list = []
            headrooms_list = []
            diff_time_loss_exit = []
            diff_delivery_rate_loss_exit = []
            diff_delivery_rate_loss_exit_even_not_exit = [] # used for ranking
            diff_delivery_rate_loss_exit_normalized_list = []
            max_delivery_rate = [] 
            delivery_rate_exit_based_log = []
            delivery_rate_at_loss_based_log = []
            max_delivery_rate_based_log = []
            diff_manual_chokepoint_exit_rate_list = []
            diff_manual_chokepoint_loss_rate_list = []
            do_not_exit_counter = 0
            samples_not_exit_list = []
            rtt_at_exit_list = []
            rtt_at_loss_list = []
            diff_rtt_loss_exit_list = []
            ##### power over bins (based on delivery rate and rtt in bins) #######
            power_at_exit_list = [] # power at exit based on delivery rate (bins)
            power_at_loss_list = [] # power at loss based on delivery rate (bins)
            # diff_power_loss_exit_list = []
            avg_power_list = []
            avg_power_prime_list = []
            max_power_before_exit_list = []
            success_exit_list = []
            success_exit_based_power_prime_list = []
            failure_exit_less_max_list = []
            failure_not_exit_list = []
            ##### power over time (based on delivery rate and rtt over time (each ack) ########
            power_at_exit_over_time_list = []
            power_at_loss_over_time_list = []
            max_power_before_exit_over_time_list = []
            avg_power_over_time_list = []
            success_exit_over_time_list = []
            failure_exit_less_max_over_time_list = []
            failure_not_exit_over_time_list = []
            power_prime_at_exit_over_time_list = []
            power_prime_at_loss_over_time_list = []
            max_power_prime_before_exit_over_time_list = []
            avg_power_prime_over_time_list = []
            success_exit_based_power_prime_over_time_list = []
            failure_exit_less_max_based_power_prime_over_time_list = []

            SERVER_IP = "130.215.28.249"
            INTERVAL = 0.01

            analysed_files = num_files

            for num in range(num_files):
                data_path = os.path.join(subfolder_path, f"log_data_testframework{num+1}.txt")

                if not os.path.exists(data_path):
                    print(f"File {data_path} does not exist")
                    analysed_files -= 1
                    continue
                
                # if num in [15, 38]:
                #     continue

                    
                print(f"Processing file: log_data_testframework{num+1}.txt")

                # Extract data from data files (.txt)
                now_list, loss_happen_list, exit_time_list, passed_bin_list, passed_bin_time_list,rtt_ms_list, \
                bytes_acked_list, time_bin_update, bin_snapshots, bin_index_list, all_bin_values, bin_index_reset, \
                all_bin_values_passed_bin_filled, exit_search_bin_index, search_do_bin_indices, norm_list, rtt_in_each_bin_update, \
                time_each_bin_filled, reset_times, bin_end_time, needed_extra_bins_to_do_search= get_data(data_path)

                # if loss does not happen, ignore the file (if there is no 1 in loss_happen_list)
                if not any(loss_happen_list):
                    print(f"File {data_path} has no loss")
                    analysed_files -= 1
                    continue
                
                ########################## Find missed bins #####################################
                missed_bins_list = np.asarray(passed_bin_list) - 1
                # Create a Pandas DataFrame
                df_missed_bins = pd.DataFrame({"time_missed_bins": passed_bin_time_list, "missed_bins": missed_bins_list})

                ######################### Find number of resets ##################################
                if len(reset_times) > 0:
                    # Find number of resets
                    total_resets = len(reset_times)
                    total_resets_list.append(total_resets)
                else:
                    total_resets = 0
                    total_resets_list.append(0)
                ######################### Find exit before loss #################################
                exit_time = exit_time_list[0] if exit_time_list else None
                loss_time = np.array(now_list)[np.array(loss_happen_list) == 1] if any(loss_happen_list) else []
                loss_time = loss_time[0] if len(loss_time) > 0 else None

                if exit_time and loss_time and exit_time < loss_time:
                    exit_before_loss = 1
                else:
                    exit_before_loss = 0

                exit_before_loss_files.append(exit_before_loss)

                # keep the exit_time for each file, if it is none, set it zero
                if exit_time:
                    exit_time_output_list.append(exit_time)
                else:
                    exit_time_output_list.append(0)

               # keep the loss_time for each file, if it is none, set it zero
                if loss_time:
                    time_of_first_loss_list.append(loss_time)
                else:
                    time_of_first_loss_list.append(now_list[-1])     

                ####################### Find rtt at exit time and loss time ######################
                rtt_at_exit_ = rtt_at_loss_ = None
                exit_indices_ = loss_indices_ = None
                if exit_time:
                    exit_indices_ = np.where(np.array(now_list) <= exit_time)[0]
                    if len(exit_indices_) > 0:
                        rtt_at_exit_ = rtt_ms_list[exit_indices_[-1]] * 1e-3 # Convert to seconds
                        rtt_at_exit_list.append(rtt_at_exit_)
                    else:
                        rtt_at_exit_list.append(None)
                else:
                    rtt_at_exit_list.append(rtt_at_exit_)

                if loss_time:
                    rtt_at_loss_ = np.asarray(rtt_ms_list)[np.array(now_list) <= loss_time]
                    loss_indices_ = np.where(np.array(now_list) <= loss_time)[0]
                    if len(rtt_at_loss_) > 0:
                        rtt_at_loss_ = rtt_at_loss_[-1] * 1e-3 # Convert to seconds
                        rtt_at_loss_list.append(rtt_at_loss_)
                    else:
                        rtt_at_loss_list.append(None)
                else:
                    rtt_at_loss_list.append(None)

                # Find diff between rtt at loss and exit
                if rtt_at_exit_ and rtt_at_loss_:
                    diff_rtt_loss_exit_ = rtt_at_loss_ - rtt_at_exit_
                    diff_rtt_loss_exit_list.append(diff_rtt_loss_exit_)
                else:
                    diff_rtt_loss_exit_list.append(None)

                if exit_indices_ is not None and loss_indices_ is not None:
                    avg_rtt_between_exit_loss = np.mean(rtt_ms_list[exit_indices_[-1]:loss_indices_[-1]])
                    avg_rtt_after_exit_before_loss_list.append(avg_rtt_between_exit_loss)

                ############### Find the delviery rate during last RTT when exit and headrooms #############
                if exit_time:
                    bytes_acked = np.array(bytes_acked_list)
                    delivery_rate = None
                    # Find RTT at exit 
                    exit_indices = np.where(np.array(now_list) <= exit_time)[0]
                    if len(exit_indices) > 0:
                        rtt_at_exit = rtt_ms_list[exit_indices[-1]] * 1e-3 # Convert to seconds
                        bytes_acked_at_exit = bytes_acked[exit_indices[-1]]

                        if loss_time:
                            time_diff = loss_time - exit_time
                            diff_over_rtt = time_diff / rtt_at_exit
                            headrooms_list.append(diff_over_rtt)
                            diff_time_loss_exit.append(time_diff)
                        else:
                            headrooms_list.append(None)
                            diff_time_loss_exit.append(None)

                        # Calculate time before RTT
                        time_pre_rtt = exit_time - rtt_at_exit
                        
                        # Find bytes acked in previous RTT safely
                        pre_rtt_indices = np.where(np.array(now_list) <= time_pre_rtt)[0]
                        if len(pre_rtt_indices) > 0:
                            # Do interpolation to find bytes acked at pre RTT
                            bytes_acked_pre_rtt0 = bytes_acked[pre_rtt_indices[-1]]
                            if pre_rtt_indices[-1] > 0:
                                bytes_acked_pre_rtt1 = bytes_acked[pre_rtt_indices[-1] - 1]
                                time_pre_rtt0 = now_list[pre_rtt_indices[-1]]
                                time_pre_rtt1 = now_list[pre_rtt_indices[-1] - 1]
                                
                                # Linear interpolation
                                bytes_acked_pre_rtt = bytes_acked_pre_rtt0 + (bytes_acked_pre_rtt1 - bytes_acked_pre_rtt0) * \
                                                      (time_pre_rtt - time_pre_rtt0) / (time_pre_rtt1 - time_pre_rtt0)
                            else:
                                bytes_acked_pre_rtt = bytes_acked_pre_rtt0
                        else:
                            bytes_acked_pre_rtt = None
                        delivery_rate = (bytes_acked_at_exit - bytes_acked_pre_rtt) / rtt_at_exit if bytes_acked_at_exit and bytes_acked_pre_rtt else None
                    else:
                        delivery_rate = None # No valid exit RTT data
                else:
                    delivery_rate = None # No exit time
                    headrooms_list.append(None)
                    diff_time_loss_exit.append(None)
                delivery_rate_at_exit_list.append(delivery_rate)   

                ################# Find the delviery rate during last RTT when the loss time ##########################
                if loss_time:
                    bytes_acked = np.array(bytes_acked_list)
                    
                    # Find RTT at loss safely
                    loss_indices = np.where(np.array(now_list) <= loss_time)[0]
                    if len(loss_indices) > 0:
                        rtt_at_loss = rtt_ms_list[loss_indices[-1]] * 1e-3 # Convert to seconds
                        bytes_acked_at_loss = bytes_acked[loss_indices[-1]]

                        # Calculate time before RTT
                        time_pre_rtt = loss_time - rtt_at_loss

                        # Find bytes acked in previous RTT safely
                        pre_rtt_indices = np.where(np.array(now_list) <= time_pre_rtt)[0]
                        if len(pre_rtt_indices) > 0:
                            bytes_acked_pre_rtt = bytes_acked[pre_rtt_indices[-1]]
                            delivery_rate_loss = (bytes_acked_at_loss - bytes_acked_pre_rtt) / rtt_at_loss if bytes_acked_at_loss and bytes_acked_pre_rtt else None
                        else:
                            delivery_rate_loss = None  # No valid pre-RTT data
                    else:
                        delivery_rate_loss = None  # No valid exit RTT data
                else:
                    delivery_rate_loss = None  # No exit time
                
                delivery_rate_At_loss_list.append(delivery_rate_loss)

                ######################## Max delivery rate between all delivery rate per ack ##################################
                rtt_s_list = np.asarray(rtt_ms_list) * 1e-3
                delivery_rates_calculated, start_index_to_cal_delv_rate, time_cal_delv_rates = calculate_delivery_rate_per_ack(bytes_acked_list, now_list, rtt_s_list)
                if delivery_rates_calculated:
                    max_rate = max(delivery_rates_calculated)
                    max_delivery_rate.append(max_rate)

                ######################### Find delivery rate at exit time and loss time ############################
                exit_index__ = loss_index__ = None
                delivery_rate_exit = delivery_rate_loss = None
                if exit_time:
                    if delivery_rates_calculated:
                        exit_index__ = np.where(np.asarray(time_cal_delv_rates) <= exit_time)[0]
                        if len(exit_index__) > 0:
                            delivery_rate_exit = delivery_rates_calculated[exit_index__[-1]]
                            delivery_rate_exit_over_time_list.append(delivery_rate_exit)
                        else:
                            delivery_rate_exit_over_time_list.append(None)
                    else:
                        delivery_rate_exit_over_time_list.append(None)
                else:
                    delivery_rate_exit_over_time_list.append(None)

                if loss_time:
                    if delivery_rates_calculated:
                        loss_index__ = np.where(np.asarray(time_cal_delv_rates) <= loss_time)[0]
                        if len(loss_index__) > 0:
                            delivery_rate_loss = delivery_rates_calculated[loss_index__[-1]]
                            delivery_rate_loss_over_time_list.append(delivery_rate_loss)
                        else:
                            delivery_rate_loss_over_time_list.append(None)
                    else:
                        delivery_rate_loss_over_time_list.append(None)

                if exit_index__ is not None and loss_index__ is not None:
                    avg_delivery_rate_between_exit_loss = np.mean(delivery_rates_calculated[exit_index__[-1]:loss_index__[-1]])
                    avg_delivery_rate_after_exit_before_loss_list.append(avg_delivery_rate_between_exit_loss)

                ##################### Delivery rate (at exit, loss, max) based on what kernel log is reported ################
                if ANALYSIS_LOG_CSV:        
                    csv_log_file = os.path.join(log_csv_path, f"log_data_testframework{num+1}.csv")
                    if os.path.exists(csv_log_file):
                        df_log = pd.read_csv(csv_log_file)
                        
                        MSS = df_log["mss"].iloc[0]
                        whole_time_log = df_log["now_us"] * 1e-6 # s
                        whole_rtt_log = df_log["rtt_us"] * 1e-6 # s
                        tp_delivered_rate = df_log["tp_deliver_rate"] * df_log["mss"].iloc[0]
                        tp_interval_us = df_log["tp_interval"]
                        loss = df_log["lost_pkt"]
                        
                        # limit data until first loss (or retrans)
                        index_first_retrans = np.where(loss>0)[0][0]
                        if index_first_retrans > 0:
                            time_log = whole_time_log[:index_first_retrans]
                            tp_delivered_rate = tp_delivered_rate[:index_first_retrans]
                            tp_interval_us = tp_interval_us[:index_first_retrans]

                            tp_delivery_rate_all = tp_delivered_rate * 8 / tp_interval_us

                            delivered_rate_at_loss_based_log = tp_delivery_rate_all.iloc[-1]  #Mb/s
                        else:
                            delivered_rate_at_loss_based_log = None

                        if exit_time:
                            index_exit = np.where(np.asarray(time_log) >= exit_time)
                            if len(index_exit) > 0:
                                index_exit_ = index_exit[0]
                            else:
                                index_exit_ = None
                            if len(index_exit_) > 0:
                                index_exit_ = index_exit_[0]
                                delivered_rate_at_exit_based_log = tp_delivery_rate_all.iloc[index_exit_] #Mb/s
                            else:
                                delivered_rate_at_exit_based_log = None
                        else:
                            delivered_rate_at_exit_based_log = None

                    
                        max_delivered_rate_based_log = tp_delivery_rate_all.max() # Mb/s

                        delivery_rate_at_loss_based_log.append(delivered_rate_at_loss_based_log)
                        delivery_rate_exit_based_log.append(delivered_rate_at_exit_based_log)
                        max_delivery_rate_based_log.append(max_delivered_rate_based_log)


                    csv_log_file_all = os.path.join(log_csv_path, f"log_data{num+1}.csv")
                    if os.path.exists(csv_log_file_all):
                        df_log_all = pd.read_csv(csv_log_file_all)

                        whole_time_log_all = df_log_all["now_us"] * 1e-6 # s
                        whole_rtt_log_all = df_log_all["rtt_us"] * 1e-6 # s
                        total_bytes_sent = df_log_all["total_sent_B"] * 1e-6 # Convert to MB
                        total_bytes_acked = df_log_all["total_delv_B"] * 1e-6 # Convert to MB


                        # plot total_sent_bytes and total_delv_bytes on one graph
                        plt.figure(figsize=(8,6))
                        plt.plot(whole_time_log_all, total_bytes_sent, marker = 'o', label="Total Sent Bytes", color="b")
                        plt.plot(whole_time_log_all, total_bytes_acked, marker = 'o', label="Total Delivered Bytes", color="g")
                        if exit_time:
                            plt.axvline(x=exit_time, color='lime', linestyle='--', label='Exit Time')
                        if loss_time:
                            plt.axvline(x=loss_time, color='r', linestyle='--', label='Loss Time')
                        if exit_time and loss_time:
                            max_event = max(exit_time, loss_time)
                            plt.xlim(0, max_event + 0.1) 
                            y_val = np.where(whole_time_log_all <= max_event)[0]
                            if len(y_val) > 0:
                                plt.ylim(-0.1, max(total_bytes_sent[y_val[-1]], total_bytes_acked[y_val[-1]]) + 2)
                        elif exit_time:
                            plt.xlim(0, exit_time + 0.1)
                            y_val = np.where(whole_time_log_all <= exit_time)[0]
                            if len(y_val) > 0:
                                plt.ylim(-0.1, max(total_bytes_sent[y_val[-1]], total_bytes_acked[y_val[-1]]) + 2)
                        elif loss_time:
                            plt.xlim(0, loss_time + 0.1)
                            y_val = np.where(whole_time_log_all <= loss_time)[0]
                            if len(y_val) > 0:
                                plt.ylim(-0.1, max(total_bytes_sent[y_val[-1]], total_bytes_acked[y_val[-1]]) + 2)
                        plt.xlabel("Time (s)", fontsize=18)
                        plt.ylabel("MBytes", fontsize=18)
                        plt.title(f"Total Sent and Delivered Bytes - {folder}", fontsize=12)
                        plt.xticks(fontsize=14)
                        plt.yticks(fontsize=14)
                        plt.legend(fontsize=14)
                        plt.savefig(os.path.join(fig_subfolder_path, f"total_sent_delivered_bytes_{num+1}.png"))
                        plt.close()

                        # set the time of total_bytes_sent in the next rtt
                        time_shifted_next_rtt = whole_time_log_all + whole_rtt_log_all 

                        # plot total_sent_bytes and total_delv_bytes in next rtt
                        plt.figure(figsize=(8,6))
                        plt.plot(time_shifted_next_rtt, total_bytes_sent, marker = 'o', label="Total Sent Bytes (shifted)", color="b")
                        plt.plot(whole_time_log_all, total_bytes_acked, marker = 'o', label="Total Delivered Bytes", color="g")  
                        if exit_time:
                            plt.axvline(x=exit_time, color='lime', linestyle='--', label='Exit Time')
                        if loss_time:
                            plt.axvline(x=loss_time, color='r', linestyle='--', label='Loss Time')
                        if exit_time and loss_time:
                            max_event = max(exit_time, loss_time)
                            plt.xlim(0, max_event + 0.1) 
                            y_val = np.where(time_shifted_next_rtt <= max_event)[0]
                            if len(y_val) > 0:
                                plt.ylim(-0.1, max(total_bytes_sent[y_val[-1]], total_bytes_acked[y_val[-1]]) + 2)
                        elif exit_time:
                            plt.xlim(0, exit_time + 0.1)
                            y_val = np.where(time_shifted_next_rtt <= exit_time)[0]
                            if len(y_val) > 0:
                                plt.ylim(-0.1, max(total_bytes_sent[y_val[-1]], total_bytes_acked[y_val[-1]]) + 2)
                        elif loss_time:
                            plt.xlim(0, loss_time + 0.1)
                            y_val = np.where(time_shifted_next_rtt <= loss_time)[0]
                            if len(y_val) > 0:
                                plt.ylim(-0.1, max(total_bytes_sent[y_val[-1]], total_bytes_acked[y_val[-1]]) + 2)
                        plt.xlabel("Time (s)", fontsize=18)
                        plt.ylabel("MBytes", fontsize=18)
                        plt.title(f"Total Sent and Delivered Bytes (time shifted to next RTT) - {folder}", fontsize=12)
                        plt.xticks(fontsize=14)
                        plt.yticks(fontsize=14)
                        plt.legend(fontsize=14)
                        plt.savefig(os.path.join(fig_subfolder_path, f"total_sent_delivered_bytes_next_rtt_{num+1}.png"))
                        plt.close()
                        
                        # Shift time to t - RTT
                        time_shifted_prev_rtt = whole_time_log_all - whole_rtt_log_all

                        # plot sent bytes and shifted delivered bytes to pre RTT
                        plt.figure(figsize=(8,6))
                        plt.plot(whole_time_log_all, total_bytes_sent, marker = 'o', label="Total Sent Bytes", color="b")
                        plt.plot(time_shifted_prev_rtt, total_bytes_acked, marker = 'o', label="Total Delivered Bytes (shifted back)", color="g")
                        if exit_time:
                            plt.axvline(x=exit_time, color='lime', linestyle='--', label='Exit Time')
                        if loss_time:
                            plt.axvline(x=loss_time, color='r', linestyle='--', label='Loss Time')
                        if exit_time and loss_time:
                            max_event = max(exit_time, loss_time)
                            plt.xlim(0, max_event + 0.1) 
                            y_val = np.where(time_shifted_prev_rtt <= max_event)[0]
                            if len(y_val) > 0:
                                plt.ylim(-0.1, max(total_bytes_sent[y_val[-1]], total_bytes_acked[y_val[-1]]) + 2)
                        elif exit_time:
                            plt.xlim(0, exit_time + 0.1)
                            y_val = np.where(time_shifted_prev_rtt <= exit_time)[0]
                            if len(y_val) > 0:
                                plt.ylim(-0.1, max(total_bytes_sent[y_val[-1]], total_bytes_acked[y_val[-1]]) + 2)
                        elif loss_time:
                            plt.xlim(0, loss_time + 0.1)
                            y_val = np.where(time_shifted_prev_rtt <= loss_time)[0]
                            if len(y_val) > 0:
                                plt.ylim(-0.1, max(total_bytes_sent[y_val[-1]], total_bytes_acked[y_val[-1]]) + 2)
                        plt.xlabel("Time (s)", fontsize=18)
                        plt.ylabel("MBytes", fontsize=18)
                        plt.title(f"Total Sent and Delivered Bytes (time shifted to previous RTT) - {folder}", fontsize=12)
                        plt.xticks(fontsize=14)
                        plt.yticks(fontsize=14)
                        plt.legend(fontsize=14)
                        plt.savefig(os.path.join(fig_subfolder_path, f"total_sent_delivered_bytes_prev_rtt_{num+1}.png"))
                        plt.close()


                        # # Interpolate sent bytes at time t - RTT
                        # interpolated_sent_prev_rtt = np.interp(
                        #     time_shifted_prev_rtt,
                        #     whole_time_log_all,
                        #     total_bytes_sent
                        # )

                        # # Compute normalized difference
                        # norm_diff = (interpolated_sent_prev_rtt - total_bytes_acked) / interpolated_sent_prev_rtt
                        # norm_diff[norm_diff < 0] = 0

                        # # Mask: only use times where t - RTT >= start of time log
                        # valid_mask = time_shifted_prev_rtt >= whole_time_log_all.iloc[0]
                        # valid_times = whole_time_log_all[valid_mask]
                        # valid_norm_diff = norm_diff[valid_mask]
                        # valid_byte_sent_pre_rtt = interpolated_sent_prev_rtt[valid_mask]

                        
                        # # Plot
                        # plt.figure(figsize=(8, 6))
                        # plt.plot(valid_times, valid_norm_diff, marker = 'o', color="m", label="Normalized Difference")
                        # if exit_time:
                        #     plt.axvline(x=exit_time, color='lime', linestyle='--', label='Exit Time')
                        # if loss_time:
                        #     plt.axvline(x=loss_time, color='r', linestyle='--', label='Loss Time')
                        # if exit_time and loss_time:
                        #     max_event = max(exit_time, loss_time)
                        #     plt.xlim(0, max_event + 0.1) 
                        #     y_val = np.where(valid_times <= max_event)[0]
                        #     if len(y_val) > 0:
                        #         plt.ylim(0, max(valid_norm_diff[:y_val[-1]]) + 0.05)
                        # elif exit_time:
                        #     plt.xlim(0, exit_time + 0.1)
                        #     y_val = np.where(valid_times <= exit_time)[0]
                        #     if len(y_val) > 0:
                        #         plt.ylim(0, max(valid_norm_diff[:y_val[-1]]) + 0.05)
                        # elif loss_time:
                        #     plt.xlim(0, loss_time + 0.1)
                        #     y_val = np.where(valid_times <= loss_time)[0]
                        #     if len(y_val) > 0:
                        #         plt.ylim(0, max(valid_norm_diff[:y_val[-1]]) + 0.05)
                        # plt.axhline(y=0.35, color='lightgray', linestyle='--', label='Threshold')
                        # plt.xlabel("Time (s)", fontsize=18)
                        # plt.ylabel("Normalized Difference", fontsize=18)
                        # plt.title(f"Normalized Difference Between Est_Sent(t-RTT) and Delivered(t) - {folder}", fontsize=12)
                        # plt.xticks(fontsize=14)
                        # plt.yticks(fontsize=14)
                        # plt.legend(fontsize=14)
                        # plt.savefig(os.path.join(fig_subfolder_path, f"norm_diff_total_sent_delivered_bytes_{num+1}.png"))
                        # plt.close()


                        # # plot delivered bytes and interpolated sent bytes at time t
                        # plt.figure(figsize=(8,6))
                        # plt.plot(whole_time_log_all, total_bytes_acked, marker = 'o', label="Total Delivered Bytes", color="g")
                        # plt.plot(valid_times, valid_byte_sent_pre_rtt, marker = 'o', label="Total Est_Sent Bytes (t-RTT)", color="b")
                        # if exit_time:
                        #     plt.axvline(x=exit_time, color='lime', linestyle='--', label='Exit Time')
                        # if loss_time:
                        #     plt.axvline(x=loss_time, color='r', linestyle='--', label='Loss Time')
                        # if exit_time and loss_time:
                        #     max_event = max(exit_time, loss_time)
                        #     plt.xlim(0, max_event + 0.1) 
                        #     y_val = np.where(valid_times <= max_event)[0]
                        #     if len(y_val) > 0:
                        #         plt.ylim(-0.1, max(valid_byte_sent_pre_rtt[y_val[-1]], total_bytes_acked[y_val[-1]]) + 2)
                        # elif exit_time:
                        #     plt.xlim(0, exit_time + 0.1)
                        #     y_val = np.where(valid_times <= exit_time)[0]
                        #     if len(y_val) > 0:
                        #         plt.ylim(-0.1, max(valid_byte_sent_pre_rtt[y_val[-1]], total_bytes_acked[y_val[-1]]) + 2)
                        # elif loss_time:
                        #     plt.xlim(0, loss_time + 0.1)
                        #     y_val = np.where(valid_times <= loss_time)[0]
                        #     if len(y_val) > 0:
                        #         plt.ylim(-0.1, max(valid_byte_sent_pre_rtt[y_val[-1]], total_bytes_acked[y_val[-1]]) + 2)
                        # plt.xlabel("Time (s)", fontsize=18)
                        # plt.ylabel("MBytes", fontsize=18)
                        # plt.title(f"Total Est_Sent (t-RTT) and Delivered Bytes (t) at time (t) - {folder}", fontsize=12)
                        # plt.xticks(fontsize=14)
                        # plt.yticks(fontsize=14)
                        # plt.legend(fontsize=14)
                        # plt.savefig(os.path.join(fig_subfolder_path, f"total_est_sent_in_prev_rtt_delivered_bytes_current_{num+1}.png"))
                        # plt.close()   


                        # simulate SEARCH
                        initial_rtt = whole_rtt_log_all.iloc[0] # Convert to seconds
                        window_size = 3.5 * initial_rtt # 3.5 times initial RTT

                        # calculate the window of delivered bytes (sliding per each data)
                        index_loss = np.where(whole_time_log_all <= loss_time)[0] if loss_time else None
                                    
                        delivered_bytes_window, sent_bytes_window, time_window, rtt_related_to_window = calculate_window_of_data(
                            whole_time_log_all, window_size, total_bytes_acked, total_bytes_sent, whole_rtt_log_all)

                        # Find sent_bytes_window in the last RTT
                        time_window_shifted_one_rtt_back = np.asarray(time_window) - np.asarray(rtt_related_to_window) # sec

                        sent_bytes_window_last_rtt_interpolated = np.interp(
                            time_window_shifted_one_rtt_back,
                            time_window,
                            sent_bytes_window)
                        

                        time_window = np.array(time_window)
                        # plot sent_bytes_window_last_rtt_interpolated and delivered_bytes_window
                        plt.figure(figsize=(8,6))
                        plt.plot(time_window, sent_bytes_window_last_rtt_interpolated, marker = 'o', label="Est_Sent Wnd Bytes (t - RTT)", color="b")
                        plt.plot(time_window, delivered_bytes_window, marker = 'o', label="Delivered Wnd Bytes", color="g")
                        if exit_time:
                            plt.axvline(x=exit_time, color='lime', linestyle='--', label='Exit Time')
                        if loss_time:
                            plt.axvline(x=loss_time, color='r', linestyle='--', label='Loss Time')
                        if exit_time and loss_time:
                            max_event = max(exit_time, loss_time)
                            plt.xlim(0, max_event + 0.1) 
                            y_val = np.where(time_window <= max_event)[0]
                            if len(y_val) > 0:
                                plt.ylim(-0.1, max(sent_bytes_window_last_rtt_interpolated[y_val[-1]], delivered_bytes_window[y_val[-1]]) + 2)
                        elif exit_time:
                            plt.xlim(0, exit_time + 0.1)
                            y_val = np.where(time_window <= exit_time)[0]
                            if len(y_val) > 0:
                                plt.ylim(-0.1, max(sent_bytes_window_last_rtt_interpolated[y_val[-1]], delivered_bytes_window[y_val[-1]]) + 2)
                        elif loss_time:
                            plt.xlim(0, loss_time + 0.1)
                            y_val = np.where(time_window <= loss_time)[0]
                            if len(y_val) > 0:
                                plt.ylim(-0.1, max(sent_bytes_window_last_rtt_interpolated[y_val[-1]], delivered_bytes_window[y_val[-1]]) + 2)
                        plt.xlabel("Time (s)", fontsize=18)
                        plt.ylabel("MBytes / 3.5init_RTT", fontsize=18)
                        plt.title(f"Est_Sent Bytes (t-RTT) and Delivered Bytes(t) at time (t) (wndw:3.5RTT) - {folder}", fontsize=12)
                        plt.xticks(fontsize=14)
                        plt.yticks(fontsize=14)
                        plt.legend(fontsize=14)
                        plt.savefig(os.path.join(fig_subfolder_path, f"estimated_sent_delivered_bytes_window_{num+1}.png"))
                        plt.close()


                        # find norm and graph
                        norm_diff_based_window = (sent_bytes_window_last_rtt_interpolated - delivered_bytes_window) / sent_bytes_window_last_rtt_interpolated

                        norm_diff_based_window[norm_diff_based_window < 0] = 0

                        # plot norm
                        plt.figure(figsize=(8, 6))  
                        plt.plot(time_window, norm_diff_based_window, marker = 'o', color="m", label="Normalized Difference")
                        if exit_time:
                            plt.axvline(x=exit_time, color='lime', linestyle='--', label='Exit Time')
                        if loss_time:
                            plt.axvline(x=loss_time, color='r', linestyle='--', label='Loss Time')
                        if exit_time and loss_time:
                            max_event = max(exit_time, loss_time)
                            plt.xlim(0, max_event + 0.1) 
                            y_val = np.where(time_window <= max_event)[0]
                            if len(y_val) > 0:
                                plt.ylim(0, max(norm_diff_based_window[y_val[-1]], 0.35) + 0.05)
                        elif exit_time:
                            plt.xlim(0, exit_time + 0.1)
                            y_val = np.where(time_window >= exit_time)[0]
                            if len(y_val) > 0:
                                plt.ylim(0, max(norm_diff_based_window[y_val[0]], 0.35) + 0.05)
                        elif loss_time:
                            plt.xlim(0, loss_time + 0.1)
                            y_val = np.where(time_window >= loss_time)[0]
                            if len(y_val) > 0:
                                plt.ylim(0, max(norm_diff_based_window[:y_val[0]]) + 0.05)
                        
                        plt.axhline(y=0.35, color='lightgray', linestyle='--', label='Threshold')

                        plt.xlabel("Time (s)", fontsize=18)
                        plt.ylabel("Normalized Difference", fontsize=18)
                        plt.title(f"Norm Between Estimated Sent and Delivered Bytes (wnd:3.5RTT) - {folder}", fontsize=12)  
                        plt.xticks(fontsize=14)
                        plt.yticks(fontsize=14)
                        plt.legend(fontsize=14)
                        plt.savefig(os.path.join(fig_subfolder_path, f"norm_diff_estimated_sent_delivered_bytes_window_{num+1}.png"))
                        plt.close()

                        # find diff total_sent_bytes and total_delv_bytes at time t
                        diff_total_sent_delv_bytes = total_bytes_sent - total_bytes_acked

                        # plot
                        plt.figure(figsize=(8, 6))
                        plt.plot(whole_time_log_all, diff_total_sent_delv_bytes, marker = 'o', color="c", label="In-flight (Sent - Delivered)")
                        if exit_time:
                            plt.axvline(x=exit_time, color='lime', linestyle='--', label='Exit Time')
                        if loss_time:
                            plt.axvline(x=loss_time, color='r', linestyle='--', label='Loss Time')
                        if exit_time and loss_time:
                            max_event = max(exit_time, loss_time)
                            plt.xlim(0, max_event + 0.1) 
                            y_val = np.where(whole_time_log_all >= max_event)[0]
                            if len(y_val) > 0:
                                plt.ylim(-0.1, max(diff_total_sent_delv_bytes[:y_val[0]]) + 0.05)
                        elif exit_time:
                            plt.xlim(0, exit_time + 0.1)
                            y_val = np.where(whole_time_log_all >= exit_time)[0]
                            if len(y_val) > 0:
                                plt.ylim(-0.1, max(diff_total_sent_delv_bytes[:y_val[0]]) + 0.05)
                        elif loss_time:
                            plt.xlim(0, loss_time + 0.1)
                            y_val = np.where(whole_time_log_all >= loss_time)[0]
                            if len(y_val) > 0:
                                plt.ylim(-0.1, max(diff_total_sent_delv_bytes[:y_val[0]]) + 0.05)
                        plt.xlabel("Time (s)", fontsize=18)
                        plt.ylabel("MBytes", fontsize=18)
                        plt.title(f"In flight over time - {folder}", fontsize=12)
                        plt.xticks(fontsize=14)
                        plt.yticks(fontsize=14)
                        plt.legend(fontsize=14)
                        plt.savefig(os.path.join(fig_subfolder_path, f"diff_total_sent_delivered_bytes_{num+1}.png"))
                        plt.close()

                    #################### Find diff between delivery rate at exit and loss ############################
                    if delivery_rate and delivered_rate_at_loss_based_log:
                        diff_delivery_rate_loss_exit_ = delivered_rate_at_loss_based_log - delivery_rate
                        diff_delivery_rate_loss_exit.append(diff_delivery_rate_loss_exit_)
                        diff_rate_normalized = diff_delivery_rate_loss_exit_ / delivered_rate_at_loss_based_log
                        diff_delivery_rate_loss_exit_normalized_list.append(diff_rate_normalized)

                        diff_delivery_rate_loss_exit_even_not_exit.append(abs(diff_delivery_rate_loss_exit_))
                    elif delivery_rate_loss:
                        diff_delivery_rate_loss_exit_even_not_exit.append(delivered_rate_at_loss_based_log)
                        do_not_exit_counter += 1
                        samples_not_exit_list.append(num)


                    # manual_chockpoint_delivery_rate = [110, None, 160, None, 50, None, None, None, 170, 240, 200, 120, None, 180, None,
                    #                250, 200, None, 230, None, 100, 120, None, 170, 250, None, None, None, 170, None,
                    #                None, None, 120, 110, 145, None, 200]
                    
                    # for index, i in enumerate(manual_chockpoint_delivery_rate):
                    #     if index == num:
                    #         if i is not None:
                    #             diff_manual_chokepoint_exit_rate = i - delivery_rate
                    #             diff_manual_chokepoint_loss_rate = i - delivered_rate_at_loss_based_log
                    #             diff_manual_chokepoint_exit_rate_list.append(diff_manual_chokepoint_exit_rate)
                    #             diff_manual_chokepoint_loss_rate_list.append(diff_manual_chokepoint_loss_rate)
                    #         else:
                    #             diff_manual_chokepoint_exit_rate_list.append(None)
                    #             diff_manual_chokepoint_loss_rate_list.append(None)
                    #         break
                    #     else:
                    #         continue
                ################ total and max missed bins for each txt file ####################################
                if len(missed_bins_list) > 0 :
                    # Find total missed bins
                    total_missed_bins.append(np.sum(missed_bins_list))

                    # Find max missed bins
                    max_missed_bins.append(np.max(missed_bins_list)) 
                else:
                    # Find total missed bins
                    total_missed_bins.append(0)

                    # Find max missed bins
                    max_missed_bins.append(0) 

                #################### Find delivery rate at each bin update ###################################
                time_start_cal_delv = now_list[start_index_to_cal_delv_rate:]
                delivery_rate_at_each_bin_update = []
                # use time dictionary for bin filled
                for i in time_each_bin_filled:
                    time_bin = time_each_bin_filled[i]
                    time_index = np.where(np.asarray(time_cal_delv_rates) == time_bin)[0]
                    if len(time_index) > 0 and time_index[0] < len(delivery_rates_calculated):
                        delivery_rate_at_each_bin_update.append(delivery_rates_calculated[time_index[0]])
                    else:
                        delivery_rate_at_each_bin_update.append(None)

                # calculate delivery rate in each bin based on bin values
                delivery_rate_in_each_bin_based_bin_values = []
                for i in range(len(all_bin_values_passed_bin_filled)):
                    if  all_bin_values_passed_bin_filled[i] is not None and i < len(time_bin_update):
                        rtt_this_bin = rtt_in_each_bin_update.get(i, None) * 1e-3 # Convert to seconds
                        if rtt_this_bin is not None:
                            time_shift_rtt = time_bin_update[i] - rtt_this_bin
                            bin_index_pre_rtt = np.where(np.asarray(time_bin_update) <= time_shift_rtt)[0]
                            if len(bin_index_pre_rtt) > 0:
                                bin_index_pre_rtt = bin_index_pre_rtt[-1]
                                delivery_rate_ = (all_bin_values_passed_bin_filled[i] - all_bin_values_passed_bin_filled[bin_index_pre_rtt]) / rtt_this_bin if all_bin_values[i] and all_bin_values[bin_index_pre_rtt] else None
                                delivery_rate_in_each_bin_based_bin_values.append(delivery_rate_)
                            else:
                                delivery_rate_in_each_bin_based_bin_values.append(None)
                        else:
                            delivery_rate_in_each_bin_based_bin_values.append(None)
                    else:
                        delivery_rate_in_each_bin_based_bin_values.append(delivery_rate_in_each_bin_based_bin_values[-1])

                ################# missed bins index #######################################
                missed_bin_indices = [i for i, val in enumerate(all_bin_values) if val is None]
                ################# Plot info per each txt file ################################################
                ################# Plot delivery rate from calculation (back to one rtt)
                if len(delivery_rates_calculated) > 0:
                    plt.figure(figsize=(8,6))
                    plt.plot(time_cal_delv_rates, delivery_rates_calculated, marker="o", linestyle="-", color="b")
                    #plot vertical dashed line for exit
                    if exit_time:
                        plt.axvline(exit_time, color="g", linestyle="--", label=f"Exit Time-based_{folder}")
                    plt.ylabel("calculated delivery rate (Mb/s)", fontsize=18)
                    plt.xlabel("time", fontsize=18)
                    plt.xticks(fontsize=14)
                    plt.yticks(fontsize=14)
                    # plt.title("Calculated delivery rate with shifting back by RTT until loss")
                    # plt.grid(True)
                    plt.legend()
                    # if exit_time:
                    #     plt.xlim(0, exit_time + 0.1)
                    #     index_exit = np.where(np.asarray(now_list[start_index_to_cal_delv_rate:]) <= exit_time)[0]
                    #     plt.ylim(-1, max(delivery_rates_calculated[:index_exit[-1]]) * 1.1)
                    plt.savefig(os.path.join(fig_subfolder_path, f"all_calculated_delivery_rate_{folder}_{num+1}.png"))
                    plt.close()

                # ################# Plot delivery rate from kernel log
                # plt.figure(figsize=(8,6))
                # plt.plot(time_log, tp_delivery_rate_all, marker="o", linestyle="-", color="purple")
                # plt.ylabel("delivery rate (Mb/s)", fontsize=18)
                # plt.xlabel("time", fontsize=18)
                # plt.xticks(fontsize=14)
                # plt.yticks(fontsize=14)
                # plt.title("Delivery rate from kernel log until loss")
                # plt.grid(True)
                # plt.savefig(os.path.join(fig_subfolder_path, f"delivery_rate_log_{num+1}.png"))
                # plt.close()

                ################## Plot rtt over time
                plt.figure(figsize=(10, 5))
                plt.plot(now_list, rtt_ms_list, marker="o", linestyle="-", color="b")
                if exit_time:
                        plt.axvline(exit_time, color="g", linestyle="--", label=f"Exit Time-based_{folder}")
                        plt.xlim(0, exit_time + 0.1)
                plt.ylabel("RTT (ms)", fontsize=18)
                plt.xlabel("Time (s)", fontsize=18)
                plt.xticks(fontsize=14)
                plt.yticks(fontsize=14)
                # plt.title("RTT over time")
                # plt.grid(True)
                plt.legend()
                plt.savefig(os.path.join(fig_subfolder_path, f"rtt_{num+1}_zoomed.png"))
                plt.close()

                # ################## Plot tp_interval over time
                # tp_interval_ms = tp_interval_us / 1e3 #ms

                # plt.figure(figsize=(8,6))
                # plt.plot(time_log, tp_interval_ms, marker="o", linestyle="-", color="darkred")
                # plt.ylabel("tp interval (ms)", fontsize=18)
                # plt.xlabel("time", fontsize=18)
                # plt.xticks(fontsize=14)
                # plt.yticks(fontsize=14)
                # plt.title("tp interval over time")
                # plt.grid(True)
                # plt.savefig(os.path.join(fig_subfolder_path, f"tp_interval_{num+1}.png"))
                # plt.close()

                # ################# Plot diff between tp_interval and rtt
                # min_len = min(len(tp_interval_ms), len(rtt_ms_list))
                # diff_interval_rtt = tp_interval_ms[:min_len] - rtt_ms_list[:min_len]

                # plt.figure(figsize=(8,6))
                # plt.plot(now_list[:min_len],diff_interval_rtt, marker="o", linestyle="-", color="purple")
                # plt.ylabel("diff(tp_interval, rtt) (ms)", fontsize=18)
                # plt.xlabel("time", fontsize=18)
                # plt.xticks(fontsize=14)
                # plt.yticks(fontsize=14)
                # plt.title("Diff(tp interval, rtt) over time")
                # plt.grid(True)
                # plt.savefig(os.path.join(fig_subfolder_path, f"diff_interval_rtt_{num+1}.png"))
                # plt.close()

                ############### Plot CDF of missed bins
                # Compute CDF
                missed_bins_sorted = np.sort(missed_bins_list)
                cdf = np.arange(1, len(missed_bins_sorted) + 1) / len(missed_bins_sorted)

                plt.figure(figsize=(10, 5))
                plt.plot(missed_bins_sorted, cdf, marker="o", linestyle="-", color="b")
                plt.xlabel("Missed Bins")
                plt.ylabel("Cumulative Distribution")
                plt.title("CDF of Missed Bins")
                # plt.grid(True)
                plt.savefig(os.path.join(fig_subfolder_path, f"cdf_missed_bins_{num+1}.png"))
                plt.close()

                ############### plot CDF of missed bins before exit
                if exit_before_loss:
                    if any(np.array(passed_bin_time_list) < exit_time):
                        missed_bins_before_exit = missed_bins_list[:np.where(np.array(passed_bin_time_list) < exit_time)[0][-1]]
                    else:
                        missed_bins_before_exit = []

                    missed_bins_sorted = np.sort(missed_bins_before_exit)
                    cdf = np.arange(1, len(missed_bins_sorted) + 1) / len(missed_bins_sorted)

                    plt.figure(figsize=(10, 5))
                    plt.plot(missed_bins_sorted, cdf, marker="o", linestyle="-", color="b")
                    plt.xlabel("Missed Bins", fontsize=18)
                    plt.ylabel("Cumulative Distribution", fontsize=18)
                    plt.xticks(fontsize=14)
                    plt.yticks(fontsize=14)
                    plt.title("CDF of Missed Bins Before Exit")
                    # plt.grid(True)
                    plt.savefig(os.path.join(fig_subfolder_path, f"cdf_missed_bins_before_exit_{num+1}.png"))
                    plt.close()

                ###################### Plot missed_bin over time
                plt.figure(figsize=(15, 5))
                plt.plot(df_missed_bins["time_missed_bins"], df_missed_bins["missed_bins"], marker="o", linestyle="-", color="b")
                # add vertical lines for exit time
                if exit_time:
                    plt.axvline(exit_time, color="r", linestyle="--", label="Exit Time")
                plt.xlabel("Time (s)", fontsize=18)
                plt.ylabel("Missed Bins", fontsize=18)
                plt.xticks(fontsize=14)
                plt.yticks(fontsize=14)
                plt.xlim(left=0)
                plt.ylim(bottom=0)
                plt.title("Missed Bins Over Time")
                # plt.grid(True)
                # plt.legend()
                plt.savefig(os.path.join(fig_subfolder_path, f"missed_bins_{num+1}.png"))
                plt.close()

                ###################### Plot CDF needed extra bins to do SEARCH
                if len(needed_extra_bins_to_do_search) > 0:
                    needed_extra_bins_sorted = np.sort(needed_extra_bins_to_do_search)
                    cdf_needed_extra_bins = np.arange(1, len(needed_extra_bins_sorted) + 1) / len(needed_extra_bins_sorted)

                    plt.figure(figsize=(10, 5))
                    plt.plot(needed_extra_bins_sorted, cdf_needed_extra_bins, marker="o", linestyle="-", color="b")
                    plt.xlabel("Needed Extra Bins to do S, fontsize=18EARCH", fontsize=18)
                    plt.ylabel("Cumulative Distribution", fontsize=18)
                    # plt.title("CDF of Needed Extra Bins to do SEARCH")
                    plt.xticks(fontsize=14)
                    plt.yticks(fontsize=14)
                    plt.xlim(left=0)
                    plt.ylim(bottom=0)
                    plt.savefig(os.path.join(fig_subfolder_path, f"cdf_needed_extra_bins_to_do_search_until_loss{num+1}.png"))
                    plt.close()

                ###################### Plot CDF needed extra bins to do SEARCH if exit doe not happen
                if len(needed_extra_bins_to_do_search) > 0 and exit_time is None:
                    needed_extra_bins_to_do_search_when_not_exit_sorted = np.sort(needed_extra_bins_to_do_search)
                    cdf_needed_extra_bins_when_not_exit = np.arange(1, len(needed_extra_bins_to_do_search_when_not_exit_sorted) + 1) / len(needed_extra_bins_to_do_search_when_not_exit_sorted)

                    plt.figure(figsize=(10, 5))
                    plt.plot(needed_extra_bins_to_do_search_when_not_exit_sorted, cdf_needed_extra_bins_when_not_exit, marker="o", linestyle="-", color="b")
                    plt.xlabel("Needed Extra Bins to do SEARCH (not exit)", fontsize=18)
                    plt.ylabel("Cumulative Distribution", fontsize=18)
                    # plt.title("CDF of Needed Extra Bins to do SEARCH (not exit)")
                    plt.xticks(fontsize=14)
                    plt.yticks(fontsize=14)
                    plt.xlim(left=0)
                    plt.ylim(bottom=0)
                    plt.savefig(os.path.join(fig_subfolder_path, f"cdf_needed_extra_bins_to_do_search_not_exit{num+1}.png"))
                    plt.close()
                # ###################### plot bin values (remove bitshifting impact) over time
                # #Prepare the matrix (rows = time steps, cols = bins)
                # matrix = np.array([[snapshot.get(i, 0.0) for i in range(25)] for snapshot in bin_snapshots])

                # # Convert times to strings for y-axis labels (or use round() if you prefer)
                # time_labels = [f"{t:.3f}s" for t in time_bin_update]
                # bin_labels = [f"Bin[{i}]" for i in range(25)]

                # fig, ax = plt.subplots(figsize=(25, 15))

                # # Show the heatmap
                # cax = ax.imshow(matrix, cmap="coolwarm", aspect="auto")

                # # Add text annotations inside each cell
                # for i in range(matrix.shape[0]):  # each row (time step)
                #     for j in range(matrix.shape[1]):  # each bin
                #         val = matrix[i, j]
                #         text = f"{val:.3f}" if val > 0 else "0"
                #         ax.text(j, i, text, ha="center", va="center", fontsize=8)

                # # Set ticks and labels
                # ax.set_xticks(np.arange(25))
                # ax.set_yticks(np.arange(len(time_labels)))
                # ax.set_xticklabels(bin_labels, rotation=45, ha="right", fontsize=9)
                # ax.set_yticklabels(time_labels, fontsize=9)

                # ax.set_xlabel("Bin Index", fontsize=12)
                # ax.set_ylabel("Update Time", fontsize=12)
                # ax.set_title("Bin Values(not scale) Over Time", fontsize=14)
                # fig.colorbar(cax, label="Bin Value (bytes acked)")

                # plt.tight_layout()
                # plt.savefig(os.path.join(fig_subfolder_path, f"bin_value_heatmap_removeimpact_bitshifting{num+1}.png"))
                # plt.close()

                ########################## Plot bin values (flatten)
                # if len(all_bin_values) > 0:
                #     plt.figure(figsize=(15, 5))
                #     if len(all_bin_values_passed_bin_filled) > 0:
                #         plt.plot(all_bin_values_passed_bin_filled, marker = "o", markerfacecolor='none', linestyle="--", linewidth=1, color="dodgerblue", label="Missed Bin")
                    
                #     plt.plot(all_bin_values, marker="o", linestyle="-", color="b", label="Bin Values")

                #     if len(bin_index_reset) > 0:
                #         for i in range(len(bin_index_reset)):
                #             plt.axvline(bin_index_reset[i], linestyle="--", color = 'r', label="Reset"  if i == 0 else "")

                #     if exit_search_bin_index > 0:
                #         plt.axvline(exit_search_bin_index, linestyle="--", color='g', label="Exit")

                #     # Highlight DO points before exit
                #     if search_do_bin_indices:
                #         if exit_search_bin_index > 0:
                #             for i, do_index in enumerate(search_do_bin_indices):
                #                 if do_index <= exit_search_bin_index:
                #                     plt.plot(do_index, all_bin_values[do_index], marker="o", color="purple", markersize=8,
                #                             label="SEARCH" if i == 0 else "")   
                #         else:
                #             for i, do_index in enumerate(search_do_bin_indices):
                #                 plt.plot(do_index, all_bin_values[do_index], marker="o", color="purple", markersize=8,
                #                         label="SEARCH" if i == 0 else "") 

                #     plt.xlabel("Bin Updates")
                #     plt.ylabel("Bin Values (Mb)")
                #     plt.title("Bins")
                #     plt.legend()
                #     plt.savefig(os.path.join(fig_subfolder_path, f"bin_values_{folder}_{num+1}.png"))
                #     plt.close()

                # ################## plot norm

                # plt.figure(figsize=(10, 5))
                # plt.plot(norm_list, marker="o", linestyle="-", color="b")
                # plt.xlabel("Bin SEARCH Updates", fontsize=18)
                # plt.ylabel("norm", fontsize=18)
                # plt.xticks(fontsize=14) 
                # plt.yticks(fontsize=14)
                # plt.title("norm over bin that SEARCH updates")
                # # plt.grid(True)
                # plt.savefig(os.path.join(fig_subfolder_path, f"norm_{folder}_{num+1}.png"))
                # plt.close()

                ################## plot bin values and norm together

                if len(all_bin_values) > 0:
                    fig, ax1 = plt.subplots(figsize=(15, 5))

                    # Plot main bin values
                    if len(all_bin_values_passed_bin_filled) > 0:
                        ax1.plot(all_bin_values_passed_bin_filled, marker="o", markerfacecolor='none', linestyle="--", linewidth=1, color="dodgerblue", label="Missed Bin")

                    ax1.plot(all_bin_values, marker="o", linestyle="-", color="b", label="Bin Values")

                    if len(bin_index_reset) > 0:
                        for i, idx in enumerate(bin_index_reset):
                            ax1.axvline(idx, linestyle="--", color='dodgerblue', label="Reset" if i == 0 else "")

                            # Annotate the reset time
                            if i < len(reset_times):
                                ax1.text(idx, ax1.get_ylim()[0]- 1.2,  # Place at the bottom of the plot
                                        f"{reset_times[i]:.3f}", rotation=90,
                                        verticalalignment='top', horizontalalignment='center',
                                        fontsize=10, color='darkblue')

                    if exit_search_bin_index > 0:
                        ax1.axvline(exit_search_bin_index, linestyle="--", color='g', label="SEARCH Exit")
                        
                        # Add delivery_rate_at_exit annotation (if exists)
                        if 'delivery_rate' in locals() and delivery_rate_exit is not None:
                            ax1.text(exit_search_bin_index -7, ax1.get_ylim()[1] * 0.98,
                                    f"Exit rate: {delivery_rate_exit:.1f} Mbps",
                                    color='green', fontsize=12, fontweight='bold', rotation=0, va='top')

                        # Add delivered_rate_at_loss_based_log annotation (if exists)
                        if 'delivered_rate_at_loss_based_log' in locals() and delivered_rate_at_loss_based_log is not None:
                            ax1.text(exit_search_bin_index -7, ax1.get_ylim()[1] * 0.91,
                                    f"Loss rate: {delivered_rate_at_loss_based_log:.1f} Mbps",
                                    color='red', fontsize=12, fontweight='bold', rotation=0, va='top')
                            
                        # Add headroom annotation (if exists)
                        if 'diff_over_rtt' in locals() and diff_over_rtt is not None:
                            ax1.text(exit_search_bin_index -7, ax1.get_ylim()[1] * 0.84,
                                    f"Headroom: {diff_over_rtt:.1f} RTT",
                                    color='darkslategrey', fontsize=12, fontweight='bold', rotation=0, va='top')    
                    # Plot purple SEARCH points
                    search_x = []
                    search_y = []

                    for i, do_index in enumerate(search_do_bin_indices):
                        # if exit_search_bin_index <= 0 or do_index <= exit_search_bin_index:
                        search_x.append(do_index)
                        search_y.append(all_bin_values[do_index])
                        ax1.plot(do_index, all_bin_values[do_index], marker="o", color="purple", markersize=8,
                                label="SEARCH Update" if i == 0 else "")

                    ax1.set_xlabel("Bin Updates")
                    ax1.set_ylabel("Bin Values (Mb)")
                    ax1.set_title("Bins")

                    # Create a second y-axis
                    ax2 = ax1.twinx()
                    ax2.plot(search_x, norm_list[:len(search_x)], color="silver", marker="x", label="Norm", linewidth=2)
                    ax2.set_ylabel("Norm", color="dimgray")
                    ax2.tick_params(axis='y', labelcolor="dimgray")
                    ax2.set_ylim([-2, 50])

                    # Combine legends
                    lines_1, labels_1 = ax1.get_legend_handles_labels()
                    lines_2, labels_2 = ax2.get_legend_handles_labels()
                    ax1.legend(lines_1 + lines_2, labels_1 + labels_2, loc="upper left")

                    # Save the merged plot
                    plt.tight_layout()
                    plt.savefig(os.path.join(fig_subfolder_path, f"merged_bin_norm_{folder}_{num+1}.png"))
                    plt.close()

                ################## plot bin values and norm together (zoomed in first SEARCH exit)

                # if len(all_bin_values) > 0:
                    # fig, ax1 = plt.subplots(figsize=(15, 5))

                    # # Plot main bin values
                    # if len(all_bin_values_passed_bin_filled) > 0:
                    #     ax1.plot(all_bin_values_passed_bin_filled, marker="o", markerfacecolor='none', linestyle="--", linewidth=1, color="dodgerblue", label="Missed Bin")

                    # ax1.plot(all_bin_values, marker="o", linestyle="-", color="b", label="Bin Values")

                    # if len(bin_index_reset) > 0:
                    #     for i, idx in enumerate(bin_index_reset):
                    #         ax1.axvline(idx, linestyle="--", color='dodgerblue', label="Reset" if i == 0 else "")

                    #         # Annotate the reset time
                    #         if i < len(reset_times):
                    #             ax1.text(idx, ax1.get_ylim()[0]- 1.2,  # Place at the bottom of the plot
                    #                     f"{reset_times[i]:.3f}", rotation=90,
                    #                     verticalalignment='top', horizontalalignment='center',
                    #                     fontsize=10, color='darkblue')
                                
                    # if exit_search_bin_index > 0:
                    #     plt.xlim(-1, exit_search_bin_index + 10)
                    #     # find y_vals and remove the None value to c an find max
                    #     y_vals = [val for val in all_bin_values[:exit_search_bin_index + 10] if val is not None]
                    #     if y_vals:
                    #         ax1.set_ylim([-0.1, max(y_vals) *2])

                    # if exit_search_bin_index > 0:
                    #     ax1.axvline(exit_search_bin_index, linestyle="--", color='g', label="SEARCH Exit")
                        
                    #     # Add delivery_rate_at_exit annotation (if exists)
                    #     if 'delivery_rate' in locals() and delivery_rate_exit is not None:
                    #         ax1.text(exit_search_bin_index -7, ax1.get_ylim()[1] * 0.98,
                    #                 f"Exit rate: {delivery_rate_exit:.1f} Mbps",
                    #                 color='green', fontsize=12, fontweight='bold', rotation=0, va='top')

                    #     # Add delivered_rate_at_loss_based_log annotation (if exists)
                    #     if 'delivered_rate_at_loss_based_log' in locals() and delivered_rate_at_loss_based_log is not None:
                    #         ax1.text(exit_search_bin_index -7, ax1.get_ylim()[1] * 0.91,
                    #                 f"Loss rate: {delivered_rate_at_loss_based_log:.1f} Mbps",
                    #                 color='red', fontsize=12, fontweight='bold', rotation=0, va='top')
                            
                    #     # Add headroom annotation (if exists)
                    #     if 'diff_over_rtt' in locals() and diff_over_rtt is not None:
                    #         ax1.text(exit_search_bin_index -7, ax1.get_ylim()[1] * 0.84,
                    #                 f"Headroom: {diff_over_rtt:.1f} RTT",
                    #                 color='darkslategrey', fontsize=12, fontweight='bold', rotation=0, va='top')    
                    # # Plot purple SEARCH points
                    # search_x = []
                    # search_y = []

                    # for i, do_index in enumerate(search_do_bin_indices):
                    #     # if exit_search_bin_index <= 0 or do_index <= exit_search_bin_index:
                    #     search_x.append(do_index)
                    #     search_y.append(all_bin_values[do_index])
                    #     ax1.plot(do_index, all_bin_values[do_index], marker="o", color="purple", markersize=8,
                    #             label="SEARCH Update" if i == 0 else "")

                    # ax1.set_xlabel("Bin Updates")
                    # ax1.set_ylabel("Bin Values (Mb)")
                    # ax1.set_title("Bins")

                    # # Create a second y-axis
                    # ax2 = ax1.twinx()
                    # ax2.plot(search_x, norm_list[:len(search_x)], color="silver", marker="x", label="Norm", linewidth=2)
                    # ax2.set_ylabel("Norm", color="dimgray")
                    # ax2.tick_params(axis='y', labelcolor="dimgray")
                    # ax2.set_ylim([-2, 50])

                    # # Combine legends
                    # lines_1, labels_1 = ax1.get_legend_handles_labels()
                    # lines_2, labels_2 = ax2.get_legend_handles_labels()
                    # ax1.legend(lines_1 + lines_2, labels_1 + labels_2, loc="upper left")

                    # # Save the merged plot
                    # plt.tight_layout()
                    # plt.savefig(os.path.join(fig_subfolder_path, f"merged_bin_norm_{folder}_{num+1}_zoomed.png"))
                    # plt.close()

                ###################### Plot RTT value for each bin update
                if len(rtt_in_each_bin_update) > 0:
                    plt.figure(figsize=(10, 5))

                    x_vals = list(rtt_in_each_bin_update.keys())
                    y_vals = list(rtt_in_each_bin_update.values())

                    # Plot full line with filled points
                    plt.plot(x_vals, y_vals, marker="o", linestyle="-", color="darkgreen", label="All Bins")

                    # Find missed bin indices (same logic as before)
                    if missed_bin_indices and max(missed_bin_indices) < len(x_vals):
                        missed_bin_indices = [i for i, val in enumerate(all_bin_values) if val is None]
                        missed_x = [x_vals[i] for i in missed_bin_indices]
                        missed_y = [y_vals[i] for i in missed_bin_indices]

                        # Overlay hollow markers for missed bins
                        plt.plot(missed_x, missed_y, marker="o", linestyle="None", markerfacecolor='white',
                                markeredgecolor="darkgreen", label="Missed Bins")

                    plt.xlabel("Bin Update", fontsize=18)
                    plt.ylabel("RTT (ms)", fontsize=18)

                    if exit_search_bin_index > 0:
                        plt.axvline(exit_search_bin_index, linestyle="--", color='g', label=f"SEARCH Exit-based_{folder}")

                    plt.xticks(fontsize=14)
                    plt.yticks(fontsize=14)
                    plt.text(0.1, 0.95, f"Initial_rtt: {rtt_ms_list[0]:.3f}", fontsize=12, color="seagreen",
                            ha="center", va="center", transform=plt.gca().transAxes)

                    # if search_do_bin_indices:
                    #     plt.xlim(-0.5, exit_search_bin_index + 10)
                    #     y_vals_plot = [val for val in y_vals[: exit_search_bin_index + 10] if val is not None]
                    #     plt.ylim(0, max(y_vals_plot) * 1.2)
                    plt.legend(loc="lower right")
                    plt.savefig(os.path.join(fig_subfolder_path, f"rtt_in_each_bin_update_{num+1}.png"))
                    plt.close()


                ###################### Plot delivery rate for each bin update
                if len(delivery_rate_at_each_bin_update) > 0:
                    plt.figure(figsize=(10, 5))

                    x_vals = list(time_each_bin_filled.keys())
                    y_vals = delivery_rate_at_each_bin_update

                    # Full line with all solid points
                    plt.plot(x_vals, y_vals, marker="o", linestyle="-", color="maroon", label="All Bins")

                    # Overlay missed bins as hollow markers
                    if missed_bin_indices and max(missed_bin_indices) < len(x_vals):
                        missed_bin_indices = [i for i, val in enumerate(all_bin_values) if val is None]
                        missed_x = [x_vals[i] for i in missed_bin_indices]
                        missed_y = [y_vals[i] for i in missed_bin_indices]
                        plt.plot(missed_x, missed_y, marker="o", linestyle="None", markerfacecolor='white',
                                markeredgecolor="maroon", label="Missed Bins")

                    plt.xlabel("Bin Update", fontsize=18)
                    plt.ylabel("Delivery Rate (Mb/s)", fontsize=18)

                    # if exit_search_bin_index > 0:
                    #     plt.axvline(exit_search_bin_index, linestyle="--", color='g', label=f"SEARCH Exit-based_{folder}")
                    #     plt.xlim(-0.5, exit_search_bin_index + 10)
                    #     y_vals_plot = [val for val in y_vals[: exit_search_bin_index + 10] if val is not None]
                    #     plt.ylim(0, max(y_vals_plot) * 1.2)
                    plt.xticks(fontsize=14)
                    plt.yticks(fontsize=14)
                    plt.legend()
                    plt.savefig(os.path.join(fig_subfolder_path, f"delivery_rate_in_each_bin_update_{num+1}.png"))
                    plt.close()


                    # Temporary fig
                    # plot figure for delivery rate in each bin based on bin values
                    if len(delivery_rate_in_each_bin_based_bin_values) > 0:
                        plt.figure(figsize=(10, 5))

                        x_vals = list(time_each_bin_filled.keys())
                        y_vals = delivery_rate_in_each_bin_based_bin_values

                        if len(x_vals) != len(y_vals):
                            continue

                        # Full line with all solid points
                        plt.plot(x_vals, y_vals, marker="o", linestyle="-", color="maroon", label="All Bins")

                        # Overlay missed bins as hollow markers
                        if missed_bin_indices and max(missed_bin_indices) < len(x_vals):
                            missed_bin_indices = [i for i, val in enumerate(all_bin_values) if val is None]
                            missed_x = [x_vals[i] for i in missed_bin_indices]
                            missed_y = [y_vals[i] for i in missed_bin_indices]
                            plt.plot(missed_x, missed_y, marker="o", linestyle="None", markerfacecolor='white',
                                    markeredgecolor="maroon", label="Missed Bins")

                        plt.xlabel("Bin Update", fontsize=18)
                        plt.ylabel("Delivery Rate (Mb/s)", fontsize=18)

                        if exit_search_bin_index > 0:
                            plt.axvline(exit_search_bin_index, linestyle="--", color='g', label=f"SEARCH Exit-based_{folder}")

                        plt.xticks(fontsize=14)
                        plt.yticks(fontsize=14)
                        plt.legend()
                        plt.savefig(os.path.join(fig_subfolder_path, f"delivery_rate_in_each_bin_based_bin_values{num+1}.png"))
                        plt.close()
               
                ###################### The characteristic of each run  ########################

                ################## FInd avg, initial, and max RTT for each run #############################
                # Find Avg of RTT
                avg_rtt = np.mean(rtt_ms_list) if rtt_ms_list else None
                avg_rtt_list.append(avg_rtt)

                # Find Initial RTT
                initial_rtt = rtt_ms_list[0] if rtt_ms_list else None
                initial_rtt_files.append(initial_rtt)

                # Find Max RTT
                max_rtt = np.max(rtt_ms_list) if rtt_ms_list else None
                max_rtt_files.append(max_rtt)

                ###################### Calculate throughput over whole run from pcap files ######################
                # Initialize throughput calculation
                throughputs = []
                timestamps_thput = []
                if ANALYSIS_PCAP_FILE:
            
                    csv_file_path = os.path.join(pcap_csv_path, f"tcp_run_{num+1}.csv")

                    if os.path.exists(csv_file_path):
                        df = pd.read_csv(csv_file_path)

                        # Find the first row where the ack number is greater than 1000 (sync the time of pcap file and log file)
                        first_row = df[df['Ack number'] > 1000].iloc[0]

                        # Get the time value from the first row
                        time_first_ack = first_row['Time']

                        # remove the times before time_first_ack
                        df = df[df['Time'] >= time_first_ack]

                        df['Time'] = df['Time'] - time_first_ack

                        df_valid = df[(df["Source"] == SERVER_IP) & (df["retransmission"].isna())]
                        df_valid = df_valid.sort_values("Time")

                        start_time = df_valid["Time"].iloc[0]
                        end_time = start_time + INTERVAL


                        # Compute throughput in fixed intervals
                        while end_time <= df_valid["Time"].iloc[-1]:
                            window_data = df_valid.loc[(df_valid["Time"] >= start_time) & (df_valid["Time"] < end_time)]
                            if not window_data.empty:
                                total_bytes = window_data["Length"].sum() * 8 * 1e-6
                                throughput = total_bytes / INTERVAL
                                throughputs.append(throughput)
                                timestamps_thput.append(end_time)

                            # Move to next window
                            start_time = end_time
                            end_time = start_time + INTERVAL

                        # find median of throughput
                        if throughputs:
                            median_throughput = np.median(throughputs)
                            median_throughput_files.append(median_throughput)

                        ################# plot throughput
                        plt.figure(figsize=(10, 5))
                        plt.plot(timestamps_thput, throughputs, marker="o", linestyle="-", color="b")
                        plt.xlabel("Time (s)", fontsize=18)
                        plt.ylabel("Throughput (Mb/s)", fontsize=18)
                        if exit_time:
                            plt.axvline(exit_time, color="g", linestyle="--", label=f"Exit Time-based_{folder}")
                        if loss_time:
                            plt.axvline(loss_time, color="r", linestyle="--", label=f"Loss Time-based_{folder}")
                        plt.xticks(fontsize=14)
                        plt.yticks(fontsize=14)
                        plt.xlim([-0.01, 1])
                        plt.ylim(bottom=-10)
                        # plt.title("Throughput Over Time")
                        plt.savefig(os.path.join(fig_subfolder_path, f"throughput_{num+1}.png"))
                        plt.close()

                        ################## plot cdf ack size
                        if len(df) > 0:
                            ack_numbers = df['Ack number'].dropna().values

                            ack_numbers = ack_numbers[ack_numbers > 1000]  # Filter out 38 valus as they are not valid for ack numbers

                            # find the difference between consecutive ack numbers
                            ack_diffs = np.diff(ack_numbers)
                            # remove zero values
                            ack_diffs = ack_diffs[ack_diffs > 0]

                            # Calculate CDF
                            sorted_ack_diffs = np.sort(ack_diffs)
                            cdf = np.arange(1, len(sorted_ack_diffs) + 1) / len(sorted_ack_diffs)
                            
                            plt.figure(figsize=(10, 5))
                            plt.plot(sorted_ack_diffs, cdf, marker="o", linestyle="-", color="b")
                            plt.xlabel("Ack Size (bytes)", fontsize=18)
                            plt.ylabel("Cumulative Distribution", fontsize=18)
                            plt.xticks(fontsize=14)
                            plt.yticks(fontsize=14)
                            plt.xlim(left=0)
                            plt.ylim(bottom=0)
                            # plt.title("CDF of Ack Size")
                            plt.savefig(os.path.join(fig_subfolder_path, f"cdf_ack_size_{num+1}.png"))
                            plt.close()

                            # ack size based on packets
                            if MSS:
                                ack_diffs_packets = ack_diffs / MSS
                            else:
                                ack_diffs_packets = ack_diffs / 1448  # Default MSS value

                            # Calculate CDF for ack size based on packets
                            sorted_ack_diffs_packets = np.sort(ack_diffs_packets)
                            cdf_packets = np.arange(1, len(sorted_ack_diffs_packets) + 1) / len(sorted_ack_diffs_packets)
                            
                            plt.figure(figsize=(10, 5))
                            plt.plot(sorted_ack_diffs_packets, cdf_packets, marker="o", linestyle="-", color="b")
                            plt.xlabel("Ack Size (packets)", fontsize=18)
                            plt.ylabel("Cumulative Distribution", fontsize=18)
                            plt.xticks(fontsize=14)
                            plt.yticks(fontsize=14)
                            plt.xlim(left=0)
                            plt.ylim(bottom=0)
                            # plt.title("CDF of Ack Size (packets)")
                            plt.savefig(os.path.join(fig_subfolder_path, f"cdf_ack_size_packets_{num+1}.png"))
                            plt.close()

                            # plot cdf ack size based on bytes and packets until time loss
                            if loss_time and len(ack_diffs) > 0:
                                # Find the index of the first ack number greater than loss_time
                                index_loss = np.where(np.asarray(df['Time']) >= loss_time)[0][0]
                                
                                if index_loss > 0:
                                    ack_diffs_until_loss = ack_diffs[:index_loss]
                                    ack_diffs_packets_until_loss = ack_diffs_packets[:index_loss]

                                    # Calculate CDF for ack size until loss 
                                    sorted_ack_diffs_until_loss = np.sort(ack_diffs_until_loss)
                                    cdf_until_loss = np.arange(1, len(sorted_ack_diffs_until_loss) + 1) / len(sorted_ack_diffs_until_loss)

                                    plt.figure(figsize=(10, 5))
                                    plt.plot(sorted_ack_diffs_until_loss, cdf_until_loss, marker="o", linestyle="-", color="b")
                                    plt.xlabel("Ack Size (bytes)", fontsize=18)
                                    plt.ylabel("Cumulative Distribution", fontsize=18)
                                    plt.xticks(fontsize=14)
                                    plt.yticks(fontsize=14)
                                    plt.xlim(left=-0.1)
                                    plt.ylim(bottom=-0.01)
                                    # plt.title("CDF of Ack Size Until Loss")
                                    plt.savefig(os.path.join(fig_subfolder_path, f"cdf_ack_size_until_loss_{num+1}.png"))
                                    plt.close()

                                    # Calculate CDF for ack size based on packets until loss
                                    sorted_ack_diffs_packets_until_loss = np.sort(ack_diffs_packets_until_loss)
                                    cdf_packets_until_loss = np.arange(1, len(sorted_ack_diffs_packets_until_loss) + 1) / len(sorted_ack_diffs_packets_until_loss)
                                    
                                    plt.figure(figsize=(10, 5))
                                    plt.plot(sorted_ack_diffs_packets_until_loss, cdf_packets_until_loss, marker="o", linestyle="-", color="b")
                                    plt.xlabel("Ack Size (packets)", fontsize=18)
                                    plt.ylabel("Cumulative Distribution", fontsize=18)
                                    plt.xticks(fontsize=14)
                                    plt.yticks(fontsize=14)
                                    plt.xlim(left=-0.1)
                                    plt.ylim(bottom=-0.01)
                                    # plt.title("CDF of Ack Size (packets) Until Loss")
                                    plt.savefig(os.path.join(fig_subfolder_path, f"cdf_ack_size_packets_until_loss_{num+1}.png"))
                                    plt.close()

                                    # plot ack size based on packet over time until loss    
                                    plt.figure(figsize=(10, 5))
                                    plt.plot(df['Time'][:index_loss], ack_diffs_packets[:index_loss], marker="o", linestyle="-", color="b")
                                    if exit_time:
                                        plt.axvline(exit_time, color="g", linestyle="--", label=f"Exit Time-based_{folder}")
                                    plt.xlabel("Time (s)", fontsize=18)
                                    plt.ylabel("Ack Size (packets)", fontsize=18)
                                    plt.xticks(fontsize=14)
                                    plt.yticks(fontsize=14)
                                    plt.xlim(left=0)
                                    plt.ylim(bottom=0)
                                    # plt.title("Ack Size (packets) Over Time Until Loss")
                                    plt.savefig(os.path.join(fig_subfolder_path, f"ack_size_packets_over_time_until_loss_{num+1}.png"))
                                    plt.close()


                ####################### average throughput
                if len(throughputs) > 0:
                    avg_throughput = np.mean(throughputs)
                    avg_throughput_list.append(avg_throughput)
                ###################### average throughput until loss
                if len(throughputs) and loss_time:
                    index_loss = np.where(np.asarray(timestamps_thput) >= loss_time)[0][0]
                    if index_loss:
                        avg_throughput_until_loss = np.mean(throughputs[:index_loss])
                        avg_throughput_until_loss_list.append(avg_throughput_until_loss)
                        throughput_at_loss = throughputs[index_loss]
                        throughput_at_loss_list.append(throughput_at_loss)
                    else:
                        avg_throughput_until_loss_list.append(None) 
                        throughput_at_loss_list.append(None) 
                else:
                    avg_throughput_until_loss_list.append(None)
                    throughput_at_loss_list.append(None)
                ####################### throughput at exit
                if len(throughputs) and exit_time:
                    index_exit = np.where(np.asarray(timestamps_thput) >= exit_time)[0][0]
                    if index_exit:
                        throughput_at_exit = np.mean(throughputs[:index_exit])
                        throughput_at_exit_list.append(throughput_at_exit)
                    else:
                        throughput_at_exit_list.append(None)
                else:
                    throughput_at_exit_list.append(None)  
                    
                ####################### Calculate power metric based on delivery rate and rtt in each bin
                # Initialize power calculation
                power_based_delivery_in_bins_list = [] # power is delivery_rate / rtt 
                power_prime_based_delivery_in_bins_list = [] # power prime is delivery_rate / rtt^2

                if len(delivery_rate_at_each_bin_update) > 0 and len(rtt_in_each_bin_update) > 0:
                    delivery_rate_at_each_bin_update = np.asarray(delivery_rate_at_each_bin_update)
                    rtt_in_each_bin_update_values_sec = rtt_in_each_bin_update.values()
                    rtt_in_each_bin_update_values_sec = np.asarray(list(rtt_in_each_bin_update_values_sec)) * 1e-3  # Convert to seconds
                    # Calculate power
                    for i in range(len(delivery_rate_at_each_bin_update)):
                        if delivery_rate_at_each_bin_update[i] and rtt_in_each_bin_update_values_sec[i]:
                            power = delivery_rate_at_each_bin_update[i] / rtt_in_each_bin_update_values_sec[i]
                            power_prime = delivery_rate_at_each_bin_update[i] / (rtt_in_each_bin_update_values_sec[i] ** 2)
                            power_based_delivery_in_bins_list.append(power)
                            power_prime_based_delivery_in_bins_list.append(power_prime)
                        else:
                            power_based_delivery_in_bins_list.append(None)
                            power_prime_based_delivery_in_bins_list.append(None)

                    power_times = list(time_each_bin_filled.keys())

                # plot power based delivery rate in each bins
                if len(power_times) > 0 and len(power_based_delivery_in_bins_list) > 0:
                    plt.figure(figsize=(10, 5))
                    plt.plot(power_times, power_based_delivery_in_bins_list, marker="o", linestyle="-", color="darkolivegreen")
                    plt.xlabel("Bin Update", fontsize=18)
                    plt.ylabel("Power (Mb/s$^2$)", fontsize=18)
                    if exit_search_bin_index > 0:
                        plt.axvline(exit_search_bin_index, linestyle="--", color='g', label=f"SEARCH Exit-based_{folder}")
                    # Overlay missed bins as hollow markers
                    if missed_bin_indices and max(missed_bin_indices) < len(x_vals):
                        missed_bin_indices = [i for i, val in enumerate(all_bin_values) if val is None]
                        missed_x = [x_vals[i] for i in missed_bin_indices]
                        missed_y = [power_based_delivery_in_bins_list[i] for i in missed_bin_indices]
                        plt.plot(missed_x, missed_y, marker="o", linestyle="None", markerfacecolor='white',
                                markeredgecolor="darkolivegreen", label="Missed Bins")    
                    plt.xticks(fontsize=14)
                    plt.yticks(fontsize=14)
                    # plt.title("Power Over Time")
                    plt.savefig(os.path.join(fig_subfolder_path, f"power_based_delivery_rate_in_each_bin{num+1}.png"))
                    plt.close()

                # plot power prime based delivery rate in each bins
                if len(power_times) > 0 and len(power_prime_based_delivery_in_bins_list) > 0:
                    plt.figure(figsize=(10, 5))
                    plt.plot(power_times, power_prime_based_delivery_in_bins_list, marker="o", linestyle="-", color="purple")
                    plt.xlabel("Bin Update", fontsize=18)
                    plt.ylabel("Power Prime (Mb/s$^3$)", fontsize=18)
                    if exit_search_bin_index > 0:
                        plt.axvline(exit_search_bin_index, linestyle="--", color='g', label=f"SEARCH Exit-based_{folder}")
                    # Overlay missed bins as hollow markers
                    if missed_bin_indices and max(missed_bin_indices) < len(x_vals):
                        missed_bin_indices = [i for i, val in enumerate(all_bin_values) if val is None]
                        missed_x = [x_vals[i] for i in missed_bin_indices]
                        missed_y = [power_prime_based_delivery_in_bins_list[i] for i in missed_bin_indices]
                        plt.plot(missed_x, missed_y, marker="o", linestyle="None", markerfacecolor='white',
                                markeredgecolor="purple", label="Missed Bins")    
                    plt.xticks(fontsize=14)
                    plt.yticks(fontsize=14)
                    plt.savefig(os.path.join(fig_subfolder_path, f"power_prime_based_delivery_rate_in_each_bin{num+1}.png"))
                    plt.close()

                ####################### Average power and power prime
                if len(power_based_delivery_in_bins_list) > 0:
                    avg_power = np.mean([x for x in power_based_delivery_in_bins_list if x is not None])
                    avg_power_list.append(avg_power)
                if len(power_prime_based_delivery_in_bins_list) > 0:
                    avg_power_prime = np.mean([x for x in power_prime_based_delivery_in_bins_list if x is not None])
                    avg_power_prime_list.append(avg_power_prime)

                ####################### calculate success based on exit and power
                success = False
                if len(power_based_delivery_in_bins_list) > 0:
                    max_power = np.max([x for x in power_based_delivery_in_bins_list if x is not None])

                    # Find the index of the max_power
                    max_power_index = np.where(np.array(power_based_delivery_in_bins_list) == max_power)[0][0]
                    # power at exit
                    if exit_search_bin_index > 0:
                        power_at_exit = power_based_delivery_in_bins_list[exit_search_bin_index] if exit_search_bin_index < len(power_based_delivery_in_bins_list) else None
                        power_at_exit_list.append(power_at_exit)
                                            
                    if max_power_index and exit_search_bin_index:
                        if max_power_index <= exit_search_bin_index and exit_search_bin_index < len(power_based_delivery_in_bins_list):
                            success = True
                            Failure_exit_less_max = False
                            Failure_not_exit = False
                        else:
                            Failure_exit_less_max = True
                            success = Failure_not_exit = False
                    else:
                        success = Failure_exit_less_max = False
                        Failure_not_exit = True

                    success_exit_list.append(success) # success: True, if power at exit is greater than max power. 
                                                       # success: False, if power at exit is less than max power and exit_search_bin_index is None.

                    failure_exit_less_max_list.append(Failure_exit_less_max) # True if exit_search_bin_index is less than max_power_index
                    failure_not_exit_list.append(Failure_not_exit) # True if exit_search_bin_index is None

                ####################### Calculate  success based on power prime

                if len(power_prime_based_delivery_in_bins_list) > 0:
                    max_power_prime = np.max([x for x in power_prime_based_delivery_in_bins_list if x is not None])

                    # Find the index of the max_power
                    max_power_prime_index = np.where(np.array(power_prime_based_delivery_in_bins_list) == max_power_prime)[0][0]
                    # power at exit
                    if exit_search_bin_index > 0:
                        power_at_exit = power_prime_based_delivery_in_bins_list[exit_search_bin_index] if exit_search_bin_index < len(power_prime_based_delivery_in_bins_list) else None                        
                    if max_power_prime_index and exit_search_bin_index:
                        if max_power_prime_index <= exit_search_bin_index and exit_search_bin_index < len(power_prime_based_delivery_in_bins_list):
                            success = True
                        else:
                            success = False
                    else:
                        success = False

                    success_exit_based_power_prime_list.append(success)
                ####################### Find max power before exit
                if exit_search_bin_index > 0 and len(power_based_delivery_in_bins_list) > 0:
                    max_power_before_exit = np.max([x for x in power_based_delivery_in_bins_list[:exit_search_bin_index] if x is not None])
                    max_power_before_exit_list.append(max_power_before_exit)

                ####################### Calculate power over time based on delivery rate and rtt
                power_over_time_list = []
                power_prime_over_time_list = []
                if len(delivery_rates_calculated) > 0 and len(rtt_s_list) > 0:
                    # Calculate power
                    power_over_time = delivery_rates_calculated / rtt_s_list[start_index_to_cal_delv_rate:]

                    power_prime_over_time = delivery_rates_calculated / (rtt_s_list[start_index_to_cal_delv_rate:] ** 2)

                    power_times_over_time = now_list[start_index_to_cal_delv_rate:]

                    power_over_time_list.append(power_over_time)
                    power_prime_over_time_list.append(power_prime_over_time)

                    # find power at exit_time and loss_time
                    if exit_time and len(power_times_over_time) > 0 and len(power_over_time) > 0:
                        exit_index_for_power_over_time_ = np.where(np.array(power_times_over_time) == exit_time)[0]
                        if len(exit_index_for_power_over_time_) > 0:
                            exit_index_for_power_over_time = exit_index_for_power_over_time_[0]
                            power_at_exit_over_time = power_over_time[exit_index_for_power_over_time] if exit_index_for_power_over_time < len(power_over_time) else None
                            power_prime_at_exit_over_time = power_prime_over_time[exit_index_for_power_over_time] if exit_index_for_power_over_time < len(power_prime_over_time) else None
                            power_at_exit_over_time_list.append(power_at_exit_over_time)
                            power_prime_at_exit_over_time_list.append(power_prime_at_exit_over_time)
                        else:
                            power_at_exit_over_time_list.append(None)
                            power_prime_at_exit_over_time_list.append(None)
                    if loss_time and len(power_times_over_time) > 0 and len(power_over_time) > 0:
                        loss_index = np.where(np.array(power_times_over_time) == loss_time)[0]
                        if len(loss_index) > 0:
                            loss_index_power_over_time_ = loss_index[0]
                            power_at_loss_over_time = power_over_time[loss_index_power_over_time_] if loss_index_power_over_time_ <= len(power_over_time) else None
                            power_prime_at_loss_over_time = power_prime_over_time[loss_index_power_over_time_] if loss_index_power_over_time_ <= len(power_prime_over_time) else None
                            power_at_loss_over_time_list.append(power_at_loss_over_time)
                            power_prime_at_loss_over_time_list.append(power_prime_at_loss_over_time)
                        else:
                            power_at_loss_over_time_list.append(None)
                            power_prime_at_loss_over_time_list.append(None)

                    # plot power over time
                    plt.figure(figsize=(10, 5))
                    plt.plot(power_times_over_time, power_over_time, marker="o", linestyle="-", color="darkorange")
                    plt.xlabel("Time (s)", fontsize=18)
                    plt.ylabel("Power over ack times (Mb/s$^2$)", fontsize=18)
                    if exit_time:
                        plt.axvline(exit_time, color="g", linestyle="--", label=f"Exit Time-based_{folder}")
                    if loss_time:
                        plt.axvline(loss_time, color="r", linestyle="--", label=f"Loss Time-based_{folder}")
                    plt.xticks(fontsize=14)
                    plt.yticks(fontsize=14)
                    plt.legend()
                    # plt.title("Power Over Time")
                    plt.savefig(os.path.join(fig_subfolder_path, f"power_over_time_{num+1}.png"))
                    plt.close()

                    # plot power prime over time
                    plt.figure(figsize=(10, 5))
                    plt.plot(power_times_over_time, power_prime_over_time, marker="o", linestyle="-", color="darkorange")
                    plt.xlabel("Time (s)", fontsize=18)
                    plt.ylabel("Power Prime over ack times (Mb/s$^3$)", fontsize=18)
                    if exit_time:
                        plt.axvline(exit_time, color="g", linestyle="--", label=f"Exit Time-based_{folder}")
                    if loss_time:
                        plt.axvline(loss_time, color="r", linestyle="--", label=f"Loss Time-based_{folder}")
                    plt.xticks(fontsize=14)
                    plt.yticks(fontsize=14)
                    plt.legend()
                    # plt.title("Power Prime Over Time")
                    plt.savefig(os.path.join(fig_subfolder_path, f"power_prime_over_time_{num+1}.png"))
                    plt.close()

                ####################### Average power and power prime over time
                if len(power_over_time) > 0:
                    avg_power_over_time = np.mean([x for x in power_over_time if x is not None])
                    avg_power_over_time_list.append(avg_power_over_time)
                if len(power_prime_over_time) > 0:
                    avg_power_prime_over_time = np.mean([x for x in power_prime_over_time if x is not None])
                    avg_power_prime_over_time_list.append(avg_power_prime_over_time)

                #######################Calculate success based on power over time
                if len(power_over_time) > 0:
                    max_power_over_time = np.max([x for x in power_over_time if x is not None])

                    # Find the index of the max_power
                    max_power_over_time_index = np.where(np.array(power_over_time) == max_power_over_time)[0][0]
                    # power at exit
                    if exit_time and exit_index_for_power_over_time > 0:

                        if max_power_over_time_index <= exit_index_for_power_over_time and exit_index_for_power_over_time < len(power_over_time):
                            success = True
                            Failure_exit_less_max = False
                            Failure_not_exit = False
                        else:
                            Failure_exit_less_max = True
                            success = Failure_not_exit = False
                    else:
                        success = Failure_exit_less_max = False
                        Failure_not_exit = True

                    success_exit_over_time_list.append(success)
                    failure_exit_less_max_over_time_list.append(Failure_exit_less_max) # True if exit_search_bin_index is less than max_power_index
                    failure_not_exit_over_time_list.append(Failure_not_exit) # True if exit_search_bin_index is None

                ######################### Calculate success based on power prime over time
                if len(power_prime_over_time) > 0:
                    max_power_prime_over_time = np.max([x for x in power_prime_over_time if x is not None])

                    # Find the index of the max_power
                    max_power_prime_over_time_index = np.where(np.array(power_prime_over_time) == max_power_prime_over_time)[0][0]
                    # power at exit
                    if exit_time and exit_index_for_power_over_time > 0:
                        if max_power_prime_over_time_index <= exit_index_for_power_over_time and exit_index_for_power_over_time < len(power_prime_over_time):
                            success = True
                        else:
                            success = False
                    else:
                        success = False

                    success_exit_based_power_prime_over_time_list.append(success)
                
                ####################### Find max power before exit for power over time
                if exit_time and len(power_over_time) > 0:
                    max_power_before_exit_over_time = np.max([x for x in power_over_time[:exit_index_for_power_over_time] if x is not None])
                    max_power_before_exit_over_time_list.append(max_power_before_exit_over_time)
                
                ####################### Find max power prime before exit for power prime over time
                if exit_time and len(power_prime_over_time) > 0:
                    max_power_prime_before_exit_over_time = np.max([x for x in power_prime_over_time[:exit_index_for_power_over_time] if x is not None])
                    max_power_prime_before_exit_over_time_list.append(max_power_prime_before_exit_over_time)

                ####################### Calculate power metric (throughput / rtt)
                if len(whole_time_log) > 0 and len(whole_rtt_log) > 0:
                    rtt_times = np.array(whole_time_log) # second
                    rtt_values = np.array(whole_rtt_log) # second
                    throughput_times = np.array(timestamps_thput) # second
                    throughput_values = np.array(throughputs) # Mb/s
                    # Make an interpolation function for RTT
                    rtt_interp_func = interp1d(rtt_times, rtt_values, kind='linear', fill_value='extrapolate')

                    # Now get RTTs at throughput times
                    rtt_at_throughput_times = rtt_interp_func(throughput_times)
                    rtt_at_throughput_times = np.clip(rtt_at_throughput_times, a_min=0, a_max=None)  # 0 ms floor


                    # Now calculate power
                    power__ = throughput_values / rtt_at_throughput_times # Mb/s/s 
                    power_times__ = throughput_times # second        


                    # Plot power
                    plt.figure(figsize=(10, 5))
                    plt.plot(power_times__, power__, marker="o", linestyle="-", color="darkorchid")
                    plt.xlabel("Time (s)", fontsize=18)
                    plt.ylabel("Power (Mb/s$^2$)", fontsize=18)
                    if exit_time:
                        plt.axvline(exit_time, color="g", linestyle="--", label=f"Exit Time-based_{folder}")
                    if loss_time:
                        plt.axvline(loss_time, color="r", linestyle="--", label=f"Loss Time-based_{folder}")
                    plt.xticks(fontsize=14)
                    plt.yticks(fontsize=14)
                    plt.xlim([-0.01, 1])
                    plt.legend()
                    # plt.ylim(bottom=-10)
                    # plt.title("Power Over Time")
                    plt.savefig(os.path.join(fig_subfolder_path, f"power_{num+1}.png"))
                    plt.close()

#                         ########################## Throughput at time of loss ######################
#                         if loss_time:
#                             loss_idx = (np.array(timestamps_thput) - loss_time).argmin()
#                             throughput_at_time_loss.append(throughputs[loss_idx] if throughputs else None)
#                         else:
#                             throughput_at_time_loss.append(None)

    #         ################################ Plot delivery rates over all files of each folder (alpha) ##########################################

    #         ########################### plot cdf of delivery rate at exit time from calculated data
    #         delivery_rate_list_for_cdf = [x for x in delivery_rate_at_exi_list if x is not None]
    #         if delivery_rate_list_for_cdf:
    #             delivery_rate_sorted = np.sort(delivery_rate_list_for_cdf)
    #             cdf = np.arange(1, len(delivery_rate_sorted) + 1) / len(delivery_rate_sorted)

    #             plt.figure(figsize=(10, 5))
    #             plt.plot(delivery_rate_sorted, cdf, marker="o", linestyle="-", color="b")
    #             plt.xlabel("Delivery Rate (Mb/s)", fontsize=18)
    #             plt.ylabel("Cumulative Distribution", fontsize=18)
    #             plt.xticks(fontsize=14)
    #             plt.yticks(fontsize=14)
    #             plt.xlim(left=0)
    #             plt.ylim(bottom=0)
    #             plt.title("CDF of Delivery Rate at exit")
    #             plt.grid(True)
    #             plt.savefig(os.path.join(fig_subfolder_path, f"cdf_delivery_rate_at_exit.png"))
    #             plt.close()

            ################################ plot delivery rate at loss and delivery rate at exit for all files of this folder in one plot
            plt.figure(figsize=(10, 5))
            plt.plot(delivery_rate_At_loss_list, marker="o", linestyle="", color="r", label="Delivery Rate at Loss")
            plt.plot(delivery_rate_at_exit_list, marker="D", linestyle="", color="b", label="Delivery Rate at Exit")
            plt.plot(max_delivery_rate, marker="^", linestyle="", color="g", label="Max Delivery Rate")

            # Draw vertical lines between loss and exit
            for idx, (loss, exit_) in enumerate(zip(delivery_rate_At_loss_list, delivery_rate_at_exit_list)):
                if loss is None or exit_ is None:
                    continue
                plt.vlines(x=idx, ymin=min(loss, exit_), ymax=max(loss, exit_), colors='gray', linestyles='dotted')

            # Draw vertical lines between loss and max
            for idx, (loss, max_rate) in enumerate(zip(delivery_rate_At_loss_list, max_delivery_rate)):
                if loss is None or max_rate is None:
                    continue
                plt.vlines(x=idx, ymin=min(loss, max_rate), ymax=max(loss, max_rate), colors='gray', linestyles='dotted')

            # Axis settings
            plt.xlabel("Cases", fontsize=18)
            plt.ylabel("Delivery Rate (Mb/s)", fontsize=18)
            plt.xticks(fontsize=14)
            plt.yticks(fontsize=14)
            plt.xlim(left=-0.5)
            plt.ylim(-3)  
            plt.title("Delivery Rate at Loss and Exit and Max")
            plt.legend()
            plt.grid(True)

            # Save and close
            plt.savefig(os.path.join(fig_path, f"delivery_rate_at_loss_exit__max_{folder}.png"))
            plt.close()

            ########################## plot cdf delivery rate at exit (delivery_rate_exit_over_time_list)
            delivery_rate_exit_over_time_list = [x for x in delivery_rate_exit_over_time_list if x is not None]
            if delivery_rate_exit_over_time_list:
                delivery_rate_exit_over_time_sorted = np.sort(delivery_rate_exit_over_time_list)
                cdf = np.arange(1, len(delivery_rate_exit_over_time_sorted) + 1) / len(delivery_rate_exit_over_time_sorted)
                plt.figure(figsize=(10, 5))
                plt.plot(delivery_rate_exit_over_time_sorted, cdf, marker="o", linestyle="-", color="b")
                plt.xlabel("Delivery Rate at Exit (Mb/s)", fontsize=18)
                plt.ylabel("Cumulative Distribution", fontsize=18)
                plt.xticks(fontsize=14)
                plt.yticks(fontsize=14)
                plt.xlim(left=0)
                plt.ylim(bottom=0)
                plt.title("CDF of Delivery Rate at Exit Over Time")
                plt.savefig(os.path.join(fig_path, f"cdf_delivery_rate_exit_over_time_{folder}.png"))
                plt.close()

            ########################## plot cdf delivery rate at exit and median_throughput_files
            median_throughput_files = [x for x in median_throughput_files if x is not None]
            if median_throughput_files:
                median_throughput_sorted = np.sort(median_throughput_files)
                cdf = np.arange(1, len(median_throughput_sorted) + 1) / len(median_throughput_sorted)

                plt.figure(figsize=(10, 5))
                plt.plot(median_throughput_sorted, cdf, marker="o", linestyle="-", color="b")
                plt.xlabel("Median Throughput (Mb/s)", fontsize=18)
                plt.ylabel("Cumulative Distribution", fontsize=18)
                plt.xticks(fontsize=14)
                plt.yticks(fontsize=14)
                plt.xlim(left=0)
                plt.ylim(bottom=0)
                plt.title("CDF of Median Throughput")
                plt.savefig(os.path.join(fig_path, f"cdf_median_throughput_{folder}.png"))
                plt.close()

            ########################## plot cdf absolute norm diff (median_throughput - delivery_rate_exit_over_time_list) / median_throughput

            if median_throughput_files and delivery_rate_exit_over_time_list:
                abs_norm_diff = [
                    abs((median - delivery_rate) / median) if median and delivery_rate else None
                    for median, delivery_rate in zip(median_throughput_files, delivery_rate_exit_over_time_list)    
                ]

                abs_norm_diff = [x for x in abs_norm_diff if x is not None]

                # remove abs_norm_diff > 0.2
                abs_norm_diff = [x for x in abs_norm_diff if x <= 0.2]
                
                if abs_norm_diff:
                    abs_norm_diff_sorted = np.sort(abs_norm_diff)
                    cdf = np.arange(1, len(abs_norm_diff_sorted) + 1) / len(abs_norm_diff_sorted)

                    plt.figure(figsize=(10, 5))
                    plt.plot(abs_norm_diff_sorted, cdf, marker="o", linestyle="-", color="b")
                    plt.xlabel("Normalized Absolute Difference", fontsize=18)
                    plt.ylabel("Cumulative Distribution", fontsize=18)
                    plt.xticks(fontsize=14)
                    plt.yticks(fontsize=14)
                    plt.xlim([0, 2])
                    plt.ylim(bottom=0)
                    plt.title("CDF of Absolute Normalized Difference")
                    plt.savefig(os.path.join(fig_path, f"cdf_abs_norm_diff_median_throughput_delivery_rate_exit_{folder}_new.png"))
                    plt.close()



            ########################## plot cdf time diff between exit time and loss time
            # find cdf of diff_over_rtt
            diff_ = [x for x in diff_time_loss_exit if x is not None]
            diff_time_sorted = np.sort(diff_)
            cdf = np.arange(1, len(diff_time_sorted) + 1) / len(diff_time_sorted)

            #plot cdf
            plt.figure(figsize=(10, 5))
            plt.plot(diff_time_sorted, cdf, marker="o", linestyle="-", color="b")
            plt.xlabel("loss_time - exit_time (s)", fontsize=18)
            plt.ylabel("Cumulative Distribution", fontsize=18)
            plt.xticks(fontsize=14)
            plt.yticks(fontsize=14)
            plt.savefig(os.path.join(fig_path, f"cdf_time_diff_{folder}.png"))
            plt.close()

            
            ###################### plot cdf of headrooms
            headrooms_list = [x for x in headrooms_list if x is not None]
            diff_time_loss_exit_over_rtt_sorted = np.sort(headrooms_list)
            cdf = np.arange(1, len(diff_time_loss_exit_over_rtt_sorted) + 1) / len(diff_time_loss_exit_over_rtt_sorted)

            plt.figure(figsize=(10, 5))
            plt.plot(diff_time_loss_exit_over_rtt_sorted, cdf, marker="o", linestyle="-", color="b")
            plt.xlabel("Headrooms", fontsize=18)
            plt.ylabel("Cumulative Distribution", fontsize=18)
            plt.xticks(fontsize=14)
            plt.yticks(fontsize=14)
            plt.savefig(os.path.join(fig_path, f"cdf_headroom_{folder}.png"))
            plt.close()

            #################### plot cdf diff_delivery_rate_loss_exit
            diff_delivery_rate_loss_exit = [x for x in diff_delivery_rate_loss_exit if x is not None]
            diff_delivery_rate_loss_exit_sorted = np.sort(diff_delivery_rate_loss_exit)
            cdf = np.arange(1, len(diff_delivery_rate_loss_exit_sorted) + 1) / len(diff_delivery_rate_loss_exit_sorted)

            plt.figure(figsize=(10, 5))
            plt.plot(diff_delivery_rate_loss_exit_sorted, cdf, marker="o", linestyle="-", color="b")
            plt.xlabel("diff_delivery_rate between loss and exit (Mb/s)", fontsize=18)
            plt.ylabel("Cumulative Distribution", fontsize=18)
            plt.xticks(fontsize=14)
            plt.yticks(fontsize=14)
            plt.savefig(os.path.join(fig_path, f"cdf_diff_delivery_rate_loss_exit_{folder}.png"))
            plt.close()

                        # ##################### Sort by delivery_rate_At_loss_list descending
            # combined = list(zip(
            #     delivery_rate_At_loss_list,
            #     delivery_rate_at_exi_list,
            #     max_delivery_rate
            # ))
            # combined_sorted = sorted(combined, key=lambda x: x[0] if x[0] is not None else -float('inf'), reverse=True)
            # delivery_rate_At_loss_list, delivery_rate_at_exi_list, max_delivery_rate = zip(*combined_sorted)

            # plt.figure(figsize=(10, 5))
            # plt.plot(delivery_rate_At_loss_list, marker="o", linestyle="", color="r", label="Delivery Rate at Loss")
            # plt.plot(delivery_rate_at_exi_list, marker="D", linestyle="", color="b", label="Delivery Rate at Exit")
            # plt.plot(max_delivery_rate, marker="^", linestyle="", color="g", label="Max Delivery Rate")

            # # Draw vertical lines between loss and exit
            # for idx, (loss, exit_) in enumerate(zip(delivery_rate_At_loss_list, delivery_rate_at_exi_list)):
            #     if loss is None or exit_ is None:
            #         continue
            #     plt.vlines(x=idx, ymin=min(loss, exit_), ymax=max(loss, exit_), colors='gray', linestyles='dotted')

            # # Draw vertical lines between loss and max
            # for idx, (loss, max_rate) in enumerate(zip(delivery_rate_At_loss_list, max_delivery_rate)):
            #     if loss is None or max_rate is None:
            #         continue
            #     plt.vlines(x=idx, ymin=min(loss, max_rate), ymax=max(loss, max_rate), colors='gray', linestyles='dotted')

            # # Axis settings
            # plt.xlabel("Cases (sorted by delivery rate at loss)", fontsize=18)
            # plt.ylabel("Delivery Rate (Mb/s)", fontsize=18)
            # plt.xticks(fontsize=14)
            # plt.yticks(fontsize=14)
            # plt.xlim(left=-0.5)
            # plt.ylim(bottom=-3)  
            # plt.title("Delivery Rate at Loss and Exit and Max")
            # plt.legend()
            # plt.grid(True)

            # # Save and close
            # plt.savefig(os.path.join(fig_path, f"delivery_rate_at_loss_exit__max_{folder}_sorted.png"))
            # plt.close()


            # ########################## plot time diff between exit time and loss time (Headrooms)
            # plt.figure(figsize=(10, 5))
            # plt.plot(headrooms_list, marker="o", linestyle="", color="b", label="Headroom per RTT")
            # plt.xlabel("File Index", fontsize=18)
            # plt.ylabel("(loss_time - exit_time) / RTT", fontsize=18)
            # plt.xticks(fontsize=14)
            # plt.yticks(fontsize=14)
            # plt.xlim(left=-0.5)
            # plt.title("Time difference between loss and SEARCH exit")
            # plt.legend()
            # plt.grid(True)
            # plt.savefig(os.path.join(fig_path, f"headroom_per_RTT_{folder}.png"))
            # plt.close()
            # ###################### plot cdf diff_manual_chokepoint_exit_rate_list
            # diff_manual_chokepoint_exit_rate_list = [x for x in diff_manual_chokepoint_exit_rate_list if x is not None]
            # diff_manual_chokepoint_exit_rate_list_sorted = np.sort(diff_manual_chokepoint_exit_rate_list)
            # cdf = np.arange(1, len(diff_manual_chokepoint_exit_rate_list_sorted) + 1) / len(diff_manual_chokepoint_exit_rate_list_sorted)

            # plt.figure(figsize=(10, 5))
            # plt.plot(diff_manual_chokepoint_exit_rate_list_sorted, cdf, marker="o", linestyle="-", color="b")
            # plt.xlabel("diff manual chokepoint_rate - exit_rate (Mb/s)", fontsize=18)
            # plt.ylabel("Cumulative Distribution", fontsize=18)  
            # plt.xticks(fontsize=14)
            # plt.yticks(fontsize=14)
            # plt.savefig(os.path.join(fig_path, f"cdf_diff_manual_chokepoint_exit_rate_list_{folder}.png"))
            # plt.close()

            ####################### plot cdf diff_manual_chokepoint_loss_rate_list
            # diff_manual_chokepoint_loss_rate_list = [x for x in diff_manual_chokepoint_loss_rate_list if x is not None]
            # diff_manual_chokepoint_loss_rate_list_sorted = np.sort(diff_manual_chokepoint_loss_rate_list)
            # cdf = np.arange(1, len(diff_manual_chokepoint_loss_rate_list_sorted) + 1) / len(diff_manual_chokepoint_loss_rate_list_sorted)

            # plt.figure(figsize=(10, 5))
            # plt.plot(diff_manual_chokepoint_loss_rate_list_sorted, cdf, marker="o", linestyle="-", color="b")
            # plt.xlabel("diff manual chokepoint_rate - loss_rate (Mb/s)", fontsize=18)
            # plt.ylabel("Cumulative Distribution", fontsize=18)
            # plt.xticks(fontsize=14)
            # plt.yticks(fontsize=14)
            # plt.savefig(os.path.join(fig_path, f"cdf_diff_manual_chokepoint_loss_rate_list_{folder}.png"))
            # plt.close()


            # # Filter out None values before sorting
            # filtered_headroom = [(i, val) for i, val in enumerate(headrooms_list) if val is not None]

            # # Sort the remaining by headroom (descending)
            # sorted_headroom = sorted(filtered_headroom, key=lambda x: x[1], reverse=True)

            # # Unpack sorted results
            # sorted_indices, sorted_diff_time_loss_exit_over_rtt = zip(*sorted_headroom) if sorted_headroom else ([], [])

            # # Plot the sorted values
            # plt.figure(figsize=(10, 5))
            # plt.plot(sorted_diff_time_loss_exit_over_rtt, marker="o", linestyle="", color="b", label="Headroom per RTT")
            # plt.xlabel("Cases (sorted by headroom)", fontsize=18)
            # plt.ylabel("(loss_time - exit_time) / RTT", fontsize=18)
            # plt.xticks(fontsize=14)
            # plt.yticks(fontsize=14)
            # plt.xlim(left=-0.5)
            # plt.title("Time difference between loss and SEARCH exit (sorted)")
            # plt.legend()
            # plt.grid(True)
            # plt.savefig(os.path.join(fig_path, f"headroom_per_RTT_{folder}_sorted.png"))
            # plt.close()


            ##################### Plot delivery rate (exit, loss, Max) from log data 
            if ANALYSIS_LOG_CSV:
                # plot delivery rate at loss and delivery rate at exit for all files of this folder in one plot
                plt.figure(figsize=(10, 5))
                plt.plot(delivery_rate_at_loss_based_log, marker="o", linestyle="", color="r", label="Delivery Rate at Loss")
                plt.plot(delivery_rate_exit_based_log, marker="D", linestyle="", color="b", label="Delivery Rate at Exit")
                plt.plot(max_delivery_rate_based_log, marker="^", linestyle="", color="g", label="Max Delivery Rate")

                # Draw vertical lines between loss and exit
                for idx, (loss, exit_) in enumerate(zip(delivery_rate_at_loss_based_log, delivery_rate_exit_based_log)):
                    if loss is None or exit_ is None:
                        continue
                    plt.vlines(x=idx, ymin=min(loss, exit_), ymax=max(loss, exit_), colors='gray', linestyles='dotted')

                # Draw vertical lines between loss and max
                for idx, (loss, max_rate) in enumerate(zip(delivery_rate_at_loss_based_log, delivery_rate_exit_based_log)):
                    if loss is None or max_rate is None:
                        continue
                    plt.vlines(x=idx, ymin=min(loss, max_rate), ymax=max(loss, max_rate), colors='gray', linestyles='dotted')

                # Axis settings
                plt.xlabel("Cases", fontsize=18)
                plt.ylabel("Delivery Rate (Mb/s)", fontsize=18)
                plt.xticks(fontsize=14)
                plt.yticks(fontsize=14)
                plt.xlim(left=-0.5)
                plt.ylim(bottom=-3)  
                plt.title("Delivery Rate at Loss and Exit based tp_delivered and tp_interval")
                plt.legend()
                plt.grid(True)

                # Save and close
                plt.savefig(os.path.join(fig_path, f"delivery_rate_at_loss_exit_based_log_input_{folder}.png"))
                plt.close()
            #     ####################### Sort by delivery_rate_at_loss_based_log descending
            #     combined = list(zip(
            #         delivery_rate_at_loss_based_log,
            #         delivery_rate_exit_based_log,
            #         max_delivery_rate_based_log
            #     ))
            #     combined_sorted = sorted(combined, key=lambda x: x[0] if x[0] is not None else -float('inf'), reverse=True)
            #     delivery_rate_at_loss_based_log, delivery_rate_exit_based_log, max_delivery_rate_based_log = zip(*combined_sorted)

            #     # plot delivery rate at loss and delivery rate at exit for all files of this folder in one plot
            #     plt.figure(figsize=(10, 5))
            #     plt.plot(delivery_rate_at_loss_based_log, marker="o", linestyle="", color="r", label="Delivery Rate at Loss")
            #     plt.plot(delivery_rate_exit_based_log, marker="D", linestyle="", color="b", label="Delivery Rate at Exit")
            #     plt.plot(max_delivery_rate_based_log, marker="^", linestyle="", color="g", label="Max Delivery Rate")

            #     # Draw vertical lines between loss and exit
            #     for idx, (loss, exit_) in enumerate(zip(delivery_rate_at_loss_based_log, delivery_rate_exit_based_log)):
            #         if loss is None or exit_ is None:
            #             continue
            #         plt.vlines(x=idx, ymin=min(loss, exit_), ymax=max(loss, exit_), colors='gray', linestyles='dotted')

            #     # Draw vertical lines between loss and max
            #     for idx, (loss, max_rate) in enumerate(zip(delivery_rate_at_loss_based_log, delivery_rate_exit_based_log)):
            #         if loss is None or max_rate is None:
            #             continue
            #         plt.vlines(x=idx, ymin=min(loss, max_rate), ymax=max(loss, max_rate), colors='gray', linestyles='dotted')

            #     # Axis settings
            #     plt.xlabel("Cases (sorted by delivery rate at loss)", fontsize=18)
            #     plt.ylabel("Delivery Rate (Mb/s)", fontsize=18)
            #     plt.xticks(fontsize=14)
            #     plt.yticks(fontsize=14)
            #     plt.xlim(left=-0.5)
            #     plt.ylim(bottom=-3)  
            #     plt.title("Delivery Rate at Loss and Exit based tp_delivered and tp_interval")
            #     plt.legend()
            #     plt.grid(True)

            #     # Save and close
            #     plt.savefig(os.path.join(fig_path, f"delivery_rate_at_loss_exit_based_log_input_{folder}_sorted.png"))
            #     plt.close()

            ##################################### Plot run characteristic ##############################################
            # fig_path__alpha2 = os.path.join(fig_path, "fig_alpha2_selected")
            # os.makedirs(fig_path__alpha2, exist_ok=True)

            # ########################################### plot cdf total missed bins
            # total_missed_bins_sorted = np.sort(total_missed_bins)
            # cdf = np.arange(1, len(total_missed_bins_sorted) + 1) / len(total_missed_bins_sorted)

            # plt.figure(figsize=(10, 5))
            # plt.plot(total_missed_bins_sorted, cdf, marker="o", linestyle="-", color="b")
            # plt.xlabel("Total Missed Bins", fontsize=18)
            # plt.ylabel("Cumulative Distribution", fontsize=18)
            # plt.xticks(fontsize=14)
            # plt.yticks(fontsize=14)
            # plt.xlim(left=-0.5)
            # plt.ylim(bottom=0)
            # plt.title("CDF of Total Missed Bins")
            # plt.grid(True)
            # plt.savefig(os.path.join(fig_subfolder_path, "cdf_total_missed_bins.png"))
            # plt.close()

            # ########################################### plot avg rtt, initial rtt, max rtt on the same plot
            # if folder == "alpha2":
            #     plt.figure(figsize=(10, 5))
            #     plt.plot(range(1, len(avg_rtt_list)+1), avg_rtt_list, marker="o", linestyle="-", color="b", label="Avg RTT")
            #     plt.plot(range(1, len(avg_rtt_list)+1), initial_rtt_files, marker="o", linestyle="-", color="r", label="Initial RTT")
            #     plt.plot(range(1, len(avg_rtt_list)+1), max_rtt_files, marker="o", linestyle="-", color="g", label="Max RTT")
            #     plt.xlabel("File Index", fontsize=18)
            #     plt.ylabel("RTT (ms)", fontsize=18)
            #     plt.xticks(fontsize=14)
            #     plt.yticks(fontsize=14)
            #     plt.title("RTT Analysis")
            #     plt.legend()
            #     plt.grid(True)
            #     plt.savefig(os.path.join(fig_path__alpha2, "rtt_analysis.png"))
            #     plt.close()

    #             if ANALYSIS_PCAP_FILE:

    #                 # Plot CDF of avg throughput
    #                 avg_throughput_sorted = np.sort(avg_throughput_list)
    #                 cdf = np.arange(1, len(avg_throughput_sorted) + 1) / len(avg_throughput_sorted)

    #                 plt.figure(figsize=(10, 5))
    #                 plt.plot(avg_throughput_sorted, cdf, marker="o", linestyle="-", color="b")
    #                 plt.xlabel("Avg Throughput (Mb/s)", fontsize=18)
    #                 plt.ylabel("Cumulative Distribution",   fontsize=18)
    #                 plt.xticks(fontsize=14)
    #                 plt.yticks(fontsize=14)
    #                 plt.xlim(left=0)
    #                 plt.ylim(bottom=0)
    #                 plt.title("CDF of Avg Throughput")
    #                 plt.grid(True)
    #                 plt.savefig(os.path.join(fig_path__alpha2, "cdf_avg_throughput.png"))
    #                 plt.close()


    #                 ################################### Plot of CDF throughput at time of loss
    #                 throughput_at_time_loss_sorted = np.sort(throughput_at_time_loss)
    #                 cdf = np.arange(1, len(throughput_at_time_loss_sorted) + 1) / len(throughput_at_time_loss_sorted)

    #                 plt.figure(figsize=(10, 5))
    #                 plt.plot(throughput_at_time_loss_sorted, cdf, marker="o", linestyle="-", color="b")
    #                 plt.xlabel("Throughput at Time of Loss (Mb/s)", fontsize=18)
    #                 plt.ylabel("Cumulative Distribution", fontsize=18)
    #                 plt.xticks(fontsize=14)
    #                 plt.yticks(fontsize=14)
    #                 plt.xlim(left=0)
    #                 plt.ylim(bottom=0)
    #                 plt.title("CDF of Throughput at Time of Loss")
    #                 plt.grid(True)
    #                 plt.savefig(os.path.join(fig_path__alpha2, "cdf_throughput_at_loss.png"))
    #                 plt.close()

    #                 ####################################### Plot total_missed_bins over throughput
    #                 plt.figure(figsize=(10, 5))
    #                 plt.plot(avg_throughput_list, total_missed_bins, marker="o", linestyle="", color="b")
    #                 plt.xlabel("Avg throughput (Mb/s)", fontsize=18)
    #                 plt.ylabel("Missed Bins", fontsize=18)
    #                 plt.xticks(fontsize=14)
    #                 plt.yticks(fontsize=14)
    #                 plt.xlim(left=0)
    #                 plt.ylim(bottom=0)
    #                 plt.title("Total Missed Bins vs Avg Throughput")
    #                 plt.grid(True)
    #                 plt.savefig(os.path.join(fig_path__alpha2, "total_missed_bins_vs_throughput.png"))
    #                 plt.close()

    #             ################################################ Plot CDF time of first loss
    #             time_of_first_loss_list_sorted = np.sort(time_of_first_loss_list)
    #             cdf = np.arange(1, len(time_of_first_loss_list_sorted) + 1) / len(time_of_first_loss_list_sorted)

    #             plt.figure(figsize=(10, 5))
    #             plt.plot(time_of_first_loss_list_sorted, cdf, marker="o", linestyle="-", color="b")
    #             plt.xlabel("Time of First Loss (s)", fontsize=18)
    #             plt.ylabel("Cumulative Distribution", fontsize=18)
    #             plt.xticks(fontsize=14)
    #             plt.yticks(fontsize=14)
    #             plt.xlim(left=0)
    #             plt.ylim(bottom=0)
    #             plt.title("CDF of Time of First Loss")
    #             plt.grid(True)
    #             plt.savefig(os.path.join(fig_path__alpha2, "cdf_time_of_first_loss.png"))
    #             plt.close()


    #             ################################################# Plot total_missed_bins over inital RTT
    #             plt.figure(figsize=(10, 5))
    #             plt.plot(initial_rtt_files, total_missed_bins, marker="o", linestyle="", color="b")
    #             plt.xlabel("Initial RTT (us)", fontsize=18)
    #             plt.ylabel("Missed Bins", fontsize=18) 
    #             plt.xticks(fontsize=14)
    #             plt.yticks(fontsize=14)        
    #             plt.title("Missed Bins vs Initial RTT")
    #             plt.grid(True)
    #             plt.xlim(left=0)
    #             plt.ylim(bottom=0)
    #             plt.savefig(os.path.join(fig_path__alpha2, "missed_bins_vs_initial_rtt.png"))
    #             plt.close()

                

    #         ################## calculate percentage of exit before loss ##########################################
    #         percentage_of_exit_before_loss = np.mean(exit_before_loss_files) * 100
    #         alpha_sensitivity_percentage_exit_before_loss_dict[folder] = percentage_of_exit_before_loss

    #         ################ find average exit time #######################################################
    #         average_exit_time = np.mean([x for x in exit_time_output_list if x is not None])
    #         alpha_sensitivity_exit_time_dict[folder] = average_exit_time

            ################# Save results to dictionaries for each folder
            donot_exit_percentage_dict[folder] = (do_not_exit_counter / analysed_files) * 100
            samples_not_exit_dict[folder] = samples_not_exit_list
            diff_delivery_rate_loss_exit_dict_even_not_exit[folder] = diff_delivery_rate_loss_exit_even_not_exit # used for ranking
            exit_time_dict[folder] = exit_time_output_list
            success_exit_dict[folder] = success_exit_list
            success_exit_over_time_dict[folder] = success_exit_over_time_list
            success_exit_over_time_based_power_prime_dict[folder] = success_exit_based_power_prime_over_time_list
            

            # calculate average for exit_time, loss_time, headrooms, avg_power, avg_power_prime, avg_throughput, avg_rtt, successful_exit and delivery_rate
            avg_exit_time = np.mean([x for x in exit_time_output_list if x is not None])
            avg_exit_time_dict[folder] = avg_exit_time
            avg_loss_time = np.mean([x for x in time_of_first_loss_list if x is not None])
            avg_loss_time_dict[folder] = avg_loss_time
            avg_headroom = np.mean([x for x in headrooms_list if x is not None])
            avg_headrooms_dict[folder] = avg_headroom

            avg_power = np.mean([x for x in avg_power_list if x is not None])
            avg_avg_power_dict[folder] = avg_power
            avg_power_prime = np.mean([x for x in avg_power_prime_list if x is not None])
            avg_avg_power_prime_dict[folder] = avg_power_prime
            avg_max_power = np.mean([x for x in max_power_before_exit_list if x is not None])
            avg_max_power_before_exit_dict[folder] = avg_max_power
            avg_power_at_exit = np.mean([x for x in power_at_exit_list if x is not None])
            avg_power_at_exit_dict[folder] = avg_power_at_exit
            avg_throughput = np.mean([x for x in avg_throughput_list if x is not None])
            avg_avg_throughput_dict[folder] = avg_throughput
            avg_throughput_until_loss = np.mean([x for x in avg_throughput_until_loss_list if x is not None])
            avg_avg_throughput_until_loss_dict[folder] = avg_throughput_until_loss
            avg_throughput_at_exit = np.mean([x for x in throughput_at_exit_list if x is not None])
            avg_throughput_at_exit_dict[folder] = avg_throughput_at_exit 
            avg_rtt = np.mean([x for x in avg_rtt_list if x is not None]) / 1000 # convert to seconds
            avg_avg_rtt_dict[folder] = avg_rtt
            avg_success = np.mean([1 if x else 0 for x in success_exit_list if x is not None]) * 100
            avg_successful_exit_dict[folder] = avg_success
            avg_failure_exit_less_max_dict[folder] = np.mean([1 if x else 0 for x in failure_exit_less_max_list if x is not None]) * 100
            avg_failure_not_exit_dict[folder] = np.mean([1 if x else 0 for x in failure_not_exit_list if x is not None]) * 100
            avg_delivery_rate = np.mean([x for x in delivery_rates_calculated if x is not None])
            avg_delivery_rate_dict[folder] = avg_delivery_rate # based_on_ack
            avg_successful_exit_based_power_prime = np.mean([1 if x else 0 for x in success_exit_based_power_prime_list if x is not None]) * 100
            avg_successful_exit_based_power_prime_dict[folder] = avg_successful_exit_based_power_prime

            avg_power_over_time = np.mean([x for x in avg_power_over_time_list if x is not None])
            avg_avg_power_over_time_dict[folder] = avg_power_over_time
            avg_power_prime_over_time = np.mean([x for x in avg_power_prime_over_time_list if x is not None])
            avg_avg_power_prime_over_time_dict[folder] = avg_power_prime_over_time
            avg_max_power_before_exit_over_time = np.mean([x for x in max_power_before_exit_over_time_list if x is not None])
            avg_max_power_before_exit_over_time_dict[folder] = avg_max_power_before_exit_over_time
            avg_max_power_prime_before_exit_over_time = np.mean([x for x in max_power_prime_before_exit_over_time_list if x is not None])
            avg_max_power_prime_before_exit_over_time_dict[folder] = avg_max_power_prime_before_exit_over_time
            avg_success_over_time = np.mean([x for x in success_exit_over_time_list if x is not None]) * 100
            avg_success_exit_over_time_dict[folder] = avg_success_over_time
            avg_success_over_time_based_power_prime = np.mean([x for x in success_exit_based_power_prime_over_time_list if x is not None])
            avg_success_exit_over_time_based_power_prime_dict[folder] = avg_success_over_time_based_power_prime * 100
            avg_failure_exit_less_max_over_time = np.mean([x for x in failure_exit_less_max_over_time_list if x is not None])
            avg_failure_exit_less_max_over_time_dict[folder] = avg_failure_exit_less_max_over_time * 100
            avg_failure_not_exit_over_time = np.mean([x for x in failure_not_exit_over_time_list if x is not None])
            avg_failure_not_exit_over_time_dict[folder] = avg_failure_not_exit_over_time * 100
            avg_power_at_exit_over_time = np.mean([x for x in power_at_exit_over_time_list if x is not None])
            avg_power_at_exit_over_time_dict[folder] = avg_power_at_exit_over_time
            avg_power_at_loss_over_time = np.mean([x for x in power_at_loss_over_time_list if x is not None])
            avg_power_at_loss_over_time_dict[folder] = avg_power_at_loss_over_time
            avg_delivery_rate_at_exit_over_time = np.mean([x for x in delivery_rate_exit_over_time_list if x is not None])
            avg_delivery_rate_at_exit_over_time_dict[folder] = avg_delivery_rate_at_exit_over_time
            avg_delivery_rate_at_loss_over_time = np.mean([x for x in delivery_rate_loss_over_time_list if x is not None])
            avg_delivery_rate_at_loss_over_time_dict[folder] = avg_delivery_rate_at_loss_over_time
            avg_rtt_at_exit = np.mean([x for x in rtt_at_exit_list if x is not None])
            avg_rtt_at_exit_dict[folder] = avg_rtt_at_exit
            avg_delivery_rate_after_exit_before_loss = np.mean([x for x in avg_delivery_rate_after_exit_before_loss_list if x is not None])
            avg_avg_delivery_rate_after_exit_before_loss_dict[folder] = avg_delivery_rate_after_exit_before_loss
            avg_rtt_after_exit_before_loss = np.mean([x for x in avg_rtt_after_exit_before_loss_list if x is not None]) / 1000 # convert to seconds
            avg_avg_rtt_after_exit_before_loss_dict[folder] = avg_rtt_after_exit_before_loss
            avg_power_over_time_at_loss = np.mean([x for x in power_at_loss_over_time_list if x is not None])
            avg_power_at_loss_over_time_dict[folder] = avg_power_over_time_at_loss
            avg_throughput_at_loss = np.mean([x for x in throughput_at_loss_list if x is not None])
            avg_throughput_at_loss_dict[folder] = avg_throughput_at_loss
            avg_rtt_at_loss = np.mean([x for x in rtt_at_loss_list if x is not None])
            avg_rtt_at_loss_dict[folder] = avg_rtt_at_loss
            avg_total_missed_bins_dict[folder] = np.mean([x for x in total_missed_bins if x is not None])
            avg_num_of_resets = np.mean([x for x in total_resets_list if x is not None])
            avg_num_of_resets_dict[folder] = avg_num_of_resets

            # Make cdf for headrooms_list and save in dict
            headrooms_list = [x for x in headrooms_list if x is not None]
            diff_time_loss_exit_over_rtt_sorted = np.sort(headrooms_list)
            cdf = np.arange(1, len(diff_time_loss_exit_over_rtt_sorted) + 1) / len(diff_time_loss_exit_over_rtt_sorted)
            cdf_diff_time_loss_exit_over_rtt_dict[folder] = [diff_time_loss_exit_over_rtt_sorted, cdf]

            # Make cdf for diff_delivery_rate_loss_exit_normalized_list and save in dict    
            diff_delivery_rate_loss_exit_normalized_list = [x for x in diff_delivery_rate_loss_exit_normalized_list if x is not None]
            normalized_diff_delivery_rate_loss_exit_sorted = np.sort(diff_delivery_rate_loss_exit_normalized_list)
            cdf = np.arange(1, len(normalized_diff_delivery_rate_loss_exit_sorted) + 1) / len(normalized_diff_delivery_rate_loss_exit_sorted)
            cdf_normalized_diff_delivery_rate_loss_exit_dict[folder] = [normalized_diff_delivery_rate_loss_exit_sorted, cdf]

            # Make cdf for diff_time_loss_exit and save in dict
            diff_time_loss_exit = [x for x in diff_time_loss_exit if x is not None]
            diff_time_loss_exit_sorted = np.sort(diff_time_loss_exit)
            cdf = np.arange(1, len(diff_time_loss_exit_sorted) + 1) / len(diff_time_loss_exit_sorted)
            cdf_diff_time_loss_exit_dict[folder] = [diff_time_loss_exit_sorted, cdf]

            # Make cdf for diff_delivery_rate_loss_exit and save in dict
            diff_delivery_rate_loss_exit = [x for x in diff_delivery_rate_loss_exit if x is not None]
            diff_delivery_rate_loss_exit_sorted = np.sort(diff_delivery_rate_loss_exit)
            cdf = np.arange(1, len(diff_delivery_rate_loss_exit_sorted) + 1) / len(diff_delivery_rate_loss_exit_sorted)
            cdf_diff_delivery_rate_loss_exit_dict[folder] = [diff_delivery_rate_loss_exit_sorted, cdf]

            # Make CDF for diff_rtt_loss_exit and save in dict
            diff_rtt_loss_exit = [x for x in diff_rtt_loss_exit_list if x is not None]
            diff_rtt_loss_exit_sorted = np.sort(diff_rtt_loss_exit)
            cdf = np.arange(1, len(diff_rtt_loss_exit_sorted) + 1) / len(diff_rtt_loss_exit_sorted)
            cdf_diff_rtt_loss_exit_dict[folder] = [diff_rtt_loss_exit_sorted, cdf]

            # Make CDF for diff power at loss and exit and save in dict
            # diff_power_loss_exit = [x for x in diff_power_loss_exit_list if x is not None]
            # diff_power_loss_exit_sorted = np.sort(diff_power_loss_exit)
            # cdf = np.arange(1, len(diff_power_loss_exit_sorted) + 1) / len(diff_power_loss_exit_sorted)
            # cdf_dif_power_loss_exit_dict[folder] = [diff_power_loss_exit_sorted, cdf]

    reference_list = success_exit_over_time_dict["alpha100"]
    compare_folders = ["alpha1", "alpha2", "alpha3", "alpha4"]

    for folder in compare_folders:
        current_list = success_exit_over_time_dict[folder]
        count = 0
        total = analysed_files

        for curr, ref in zip(current_list, reference_list):
            if curr is True:
                if ref is not True:  # ref is False or None
                    count += 1

        percentage = 100 * count / total if total > 0 else 0
        success_when_ref_failed_over_time[folder] = percentage
    # add None for folder alpha100
    success_when_ref_failed_over_time["alpha100"] = None

    # Combine all data into a dictionary of rows
    data_link_char = {
        'folder': list(avg_avg_rtt_dict.keys()),
        'avg_loss_time (s)': list(avg_loss_time_dict.values()),
        # 'avg_throughput (Mb/s)': list(avg_avg_throughput_dict.values()),
        'avg_throughput until loss (Mb/s)': list(avg_avg_throughput_until_loss_dict.values()),
        'avg_power at loss ': list(avg_power_at_loss_over_time_dict.values()),
        'avg_rtt (s)': list(avg_avg_rtt_dict.values()),
        'avg_delivery_rate (Mb/s)': list(avg_delivery_rate_dict.values()),
    }

    data_success_info = {
        'folder': list(avg_exit_time_dict.keys()),
        'avg_successful_exit (%)': list(avg_success_exit_over_time_dict.values()),
        'avg_failure (Early) (%)': list(avg_failure_exit_less_max_over_time_dict.values()),
        'avg_failure (Late) (%)': list(avg_failure_not_exit_over_time_dict.values()),
        'success_exit_when_ref_failed [ref: alpha100] (%)': list(success_when_ref_failed_over_time.values()),
        'avg_successful_exit_based_power_prime (%)': list(avg_success_exit_over_time_based_power_prime_dict.values()),
    }

    data = {
        'folder': list(avg_exit_time_dict.keys()),
        'avg_exit_time (s)': list(avg_exit_time_dict.values()),
        'avg_headroom': list(avg_headrooms_dict.values()),
        'avg_max_power (before exit)': list(avg_max_power_before_exit_over_time_dict.values()),
        'avg_power_at_exit': list(avg_power_at_exit_over_time_dict.values()),
        'avg_power_at_loss': list(avg_power_at_loss_over_time_dict.values()),
        'avg_throughput_at_exit (Mb/s)': list(avg_throughput_at_exit_dict.values()),
        'avg_throughput_at_loss (Mb/s)': list(avg_throughput_at_loss_dict.values()),
        'avg_delivery_rate_at_exit (Mb/s)': list(avg_delivery_rate_at_exit_over_time_dict.values()),
        'avg_delivery_rate_after_exit_before_loss (Mb/s)': list(avg_avg_delivery_rate_after_exit_before_loss_dict.values()),
        'avg_delivery_rate_at_loss (Mb/s)': list(avg_delivery_rate_at_loss_over_time_dict.values()),
        'avg_rtt_at_exit (s)': list(avg_rtt_at_exit_dict.values()),
        'avg_rtt_after_exit_before_loss (s)': list(avg_avg_rtt_after_exit_before_loss_dict.values()),
        'avg_rtt_at_loss (s)': list(avg_rtt_at_loss_dict.values()),
        'avg_num_of_missed_bins': list(avg_total_missed_bins_dict.values()),
        'avg_num_of_resets': list(avg_num_of_resets_dict.values()),
    }

    # Create DataFrame
    df_link_char = pd.DataFrame(data_link_char)
    df_success_info = pd.DataFrame(data_success_info)
    df = pd.DataFrame(data)

    # Sort by folder name if needed
    # df_link_char['folder_num'] = df_link_char['folder'].str.extract(r'alpha(\d+)').astype(int)    
    # df_link_char = df_link_char.sort_values(by='folder_num')
    # df['folder_num'] = df['folder'].str.extract(r'alpha(\d+)').astype(int)
    # df = df.sort_values(by='folder_num')
    # df_success_info['folder_num'] = df_success_info['folder'].str.extract(r'alpha(\d+)').astype(int)
    # df_success_info = df_success_info.sort_values(by='folder_num')

    # Save to CSV
    df_link_char.to_csv(os.path.join(fig_path, "link_characteristics.csv"), index=False, float_format="%.4f")
    df.to_csv(os.path.join(fig_path, "averages_summary.csv"), index=False, float_format="%.4f")
    df_success_info.to_csv(os.path.join(fig_path, "success_info.csv"), index=False, float_format="%.4f")

        # Load CSV and convert to HTML table
    df_html = pd.read_csv(os.path.join(fig_path, "averages_summary.csv"))
    table_html = df_html.to_html(index=False, float_format="%.2f", border=1)

    df_link_html = pd.read_csv(os.path.join(fig_path, "link_characteristics.csv"))
    table_link_html = df_link_html.to_html(index=False, float_format="%.2f", border=1)

    df_success_info_html = pd.read_csv(os.path.join(fig_path, "success_info.csv"))
    table_success_info_html = df_success_info_html.to_html(index=False, float_format="%.2f", border=1)

    # Save the HTML table to a file
    with open(os.path.join(fig_path,"averages_summary.html"), "w", encoding="utf-8") as f:
        f.write(table_html)
    with open(os.path.join(fig_path,"link_characteristics.html"), "w", encoding="utf-8") as f:
        f.write(table_link_html)
    with open(os.path.join(fig_path,"success_info.html"), "w", encoding="utf-8") as f:
        f.write(table_success_info_html)

    ############################# Plot CDF of diffs for all forlders
    ############################# Plot cdf power_at_exit for all folders in one graph
    # plt.figure(figsize=(10, 6))
    # for folder, (sorted_values, cdf) in cdf_dif_power_loss_exit_dict.items():
    #     plt.plot(sorted_values, cdf, marker='o', label=folder)
    # plt.xlabel("Diff Power between loss and exit (Mb/s/s)", fontsize=18)
    # plt.ylabel("Cumulative Distribution", fontsize=18)
    # plt.xticks(fontsize=14)
    # plt.yticks(fontsize=14)
    # plt.legend()
    # # plt.title("CDF of Power at exit")
    # plt.savefig(os.path.join(fig_path, "cdf_diff_power_loss_exit.png"))
    # plt.close()
    ############################# Plot cdf diff_delivery_rate_loss_exit for all folders in one graph
    plt.figure(figsize=(10, 6))
    for folder, (sorted_values, cdf) in cdf_diff_delivery_rate_loss_exit_dict.items():
        plt.plot(sorted_values, cdf, marker='o', label=folder)
    plt.xlabel("diff_delivery_rate (loss - exit)", fontsize=18)
    plt.ylabel("Cumulative Distribution", fontsize=18)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.legend()
    # plt.title("CDF of diff_delivery_rate between loss and exit")
    plt.savefig(os.path.join(fig_path, "cdf_diff_delivery_rate_loss_exit.png"))
    plt.close()

    ########################### Plot cdf normalized_diff_delivery_rate_loss_exit for all folders in one graph
    plt.figure(figsize=(10, 6))
    for folder, (sorted_values, cdf) in cdf_normalized_diff_delivery_rate_loss_exit_dict.items():
        plt.plot(sorted_values, cdf, marker='o', label=folder)
    plt.xlabel("normalized_diff delivery_rate (loss - exit / loss) )", fontsize=18)
    plt.ylabel("Cumulative Distribution", fontsize=18)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.legend()
    # plt.title("CDF of diff_delivery_rate between loss and exit")
    plt.savefig(os.path.join(fig_path, "cdf_normalized_diff_delivery_rate_loss_exit.png"))
    plt.close()

    #Zoomed
    plt.figure(figsize=(10, 6))
    for folder, (sorted_values, cdf) in cdf_normalized_diff_delivery_rate_loss_exit_dict.items():
        plt.plot(sorted_values, cdf, marker='o', label=folder)
    plt.xlabel("normalized_diff delivery_rate (loss - exit / loss)", fontsize=18)
    plt.ylabel("Cumulative Distribution", fontsize=18)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    # plt.xlim(right=10)
    plt.legend()
    # plt.title("CDF of diff_delivery_rate between loss and exit")
    plt.savefig(os.path.join(fig_path, "cdf_normalized_diff_delivery_rate_loss_exit_zoomed.png"))
    plt.close()

    #################################### Plot cdf diff_rtt_loss_exit for all folders in one graph
    plt.figure(figsize=(10, 6))
    for folder, (sorted_values, cdf) in cdf_diff_rtt_loss_exit_dict.items():
        plt.plot(sorted_values, cdf, marker='o', label=folder)
    plt.xlabel("diff_rtt (loss - exit)", fontsize=18)
    plt.ylabel("Cumulative Distribution", fontsize=18)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.legend()
    # plt.title("CDF of diff_rtt between loss and exit")
    plt.savefig(os.path.join(fig_path, "cdf_diff_rtt_loss_exit.png"))
    plt.close()

    ########################### Plot cdf diff_time_loss_exit for all folders in one graph
    plt.figure(figsize=(10, 6))
    for folder, (sorted_values, cdf) in cdf_diff_time_loss_exit_dict.items():
        plt.plot(sorted_values, cdf, marker = 'o' ,label=folder)
    plt.xlabel("diff_time between loss and exit (s)", fontsize=18)
    plt.ylabel("Cumulative Distribution", fontsize=18)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.legend()
    # plt.title("CDF of diff_time between loss and exit")
    plt.savefig(os.path.join(fig_path, "cdf_diff_time_loss_exit.png"))
    plt.close()

    ########################### Plot cdf diff_time_loss_exit_over_rtt for all folders in one plot
    plt.figure(figsize=(10, 6))
    for folder, (sorted_values, cdf) in cdf_diff_time_loss_exit_over_rtt_dict.items():
        plt.plot(sorted_values, cdf,marker="o", label=folder)
    plt.xlabel("Headrooms (loss_time - exit_time) / RTT", fontsize=18)
    plt.ylabel("Cumulative Distribution", fontsize=18)
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.legend()
    # plt.title("CDF of Headrooms")
    plt.savefig(os.path.join(fig_path, "cdf_headroom.png"))
    plt.close()


    ############################# Find how many cases has same exit for all alphas
    num_samples = len(exit_time_dict['alpha1'])
    same_count = 0

    for i in range(num_samples):
        exit_times = [exit_time_dict[alpha][i] for alpha in folder_names]
        if all(x == exit_times[0] for x in exit_times):
            same_count += 1

    same_percentage = same_count / num_samples * 100
    print(f"Percentage of cases with same exit time for all alphas: {same_percentage:.2f}%")

    plt.figure(figsize=(5, 5))
    plt.pie([same_count, num_samples - same_count],
        labels=['Same', 'Different'],
        autopct='%1.1f%%')
    # plt.title("Samples with Same exit_time Across Alphas[1 4]")
    plt.savefig(os.path.join(fig_path, "same_exit_time_across_all_alphas.png"))
    plt.close()

    ############################# Find how many cases for each alpha does not exit and plot it
    plt.figure(figsize=(5, 5))
    for alpha in folder_names:
        no_exit_count = donot_exit_percentage_dict[alpha]
        plt.bar(alpha, no_exit_count, label=alpha)
        plt.text(alpha, no_exit_count, f"{no_exit_count:.2f}%", ha='center', va='bottom')
    plt.ylabel("Percentage of No Exit (%)")
    plt.xlabel("Alpha")
    # plt.title("Percentage of No Exit for Each Alpha")
    plt.ylim(0, 100)
    plt.savefig(os.path.join(fig_path, "percentage_no_exit.png"))
    plt.close()


    ############################### Rank alphas by loss_rate - exit_rate
    alphas = folder_names
    num_samples = len(diff_delivery_rate_loss_exit_dict_even_not_exit['alpha1'])

    # # Try different method: 'average', 'dense', 'ordinal'
    # for method in ['average', 'dense', 'ordinal']:
    #     rank_result_alt = {alpha: [] for alpha in alphas}
    #     for i in range(num_samples):
    #         diffs = [diff_delivery_rate_loss_exit_dict_even_not_exit[alpha][i] for alpha in alphas]
    #         ranks = rankdata(diffs, method=method)
    #         for j, alpha in enumerate(alphas):
    #             rank_result_alt[alpha].append(ranks[j])
        
    #     df_alt = pd.DataFrame(rank_result_alt)

    #     # Plot it
    #     plt.figure(figsize=(8, 6))
    #     df_alt.boxplot()
    #     plt.title(f"Rank Distribution with method='{method}'")
    #     plt.ylabel("Rank")
    #     plt.xlabel("Alpha")
    #     plt.xticks(rotation=45)
    #     plt.grid(False)
    #     plt.savefig(os.path.join(fig_path, f"rank_distribution_{method}.png"))
    #     plt.close()

    # Store ranks per alpha
    rank_result = {alpha: [] for alpha in alphas}
    # Compute rank for each sample (row-wise)
    for i in range(num_samples):
        diffs = [diff_delivery_rate_loss_exit_dict_even_not_exit[alpha][i] for alpha in alphas]
        ranks = rankdata(diffs, method='min')  # lower diff = better (rank 1)
        all_diff_same = len(set(diffs)) == 1

        # Check if this index is in the "not_exit" list for **every** alpha
        all_in_not_exit = all(
            i in samples_not_exit_dict.get(alpha, []) for alpha in alphas
        )

        # If both conditions met, skip ranking this sample
        if all_diff_same and all_in_not_exit:
            continue
        for j, alpha in enumerate(alphas):
            rank_result[alpha].append(ranks[j])

    # Convert to DataFrame for analysis
    df_ranks = pd.DataFrame(rank_result)

    # Boxplot of ranks
    plt.figure(figsize=(8, 6))
    df_ranks.boxplot()
    plt.title("Rank Distribution Based on (loss_rate - exit_rate)")
    plt.ylabel("Rank (1 = best)")
    plt.xlabel("Alpha")
    plt.xticks(rotation=45)
    plt.grid(False)
    plt.savefig(os.path.join(fig_path, "rank_distribution.png"))
    plt.close()

    # Rank Frequencies per Alpha
    rank_freq = df_ranks.apply(lambda col: col.value_counts().sort_index())
    rank_freq.plot(kind='bar', figsize=(8, 6))
    plt.title("Rank Frequencies by Alpha")
    plt.xlabel("Rank")
    plt.ylabel("Count")
    plt.xticks(rotation=0)
    plt.savefig(os.path.join(fig_path, "rank_frequencies.png"))
    plt.close()


    # ############################ Percentage of Exit Before Loss (Sorted) #################################################
    # ########################### Plot % before loss
    # sorted_items = sorted(alpha_sensitivity_percentage_exit_before_loss_dict.items(), key=lambda x: float(re.search(r"alpha([0-9.]+)", x[0]).group(1)))
    # x_alpha = [float(re.search(r"alpha([0-9.]+)", k).group(1)) for k, _ in sorted_items]
    # y_percent_exit = [v for _, v in sorted_items]

    # plt.figure(figsize=(10, 5))
    # plt.scatter(x_alpha, y_percent_exit, color="b")
    # plt.xlabel("Alpha Value", fontsize=18)
    # plt.ylabel("Percentage of Exit Before Loss (%)", fontsize=18)
    # plt.xticks(fontsize=14)
    # plt.yticks(fontsize=14)
    # plt.xlim(left=0)
    # plt.ylim(bottom=0)
    # plt.title("Percentage of Exit Before Loss")
    # plt.grid(True)
    # plt.savefig(os.path.join(fig_path, "percentage_exit_before_loss.png"))
    # plt.close()

    # ################### Average Exit Time (Sorted) ##################################################################
    # ########################### PlotAvg exit

    # sorted_exit_items = sorted(alpha_sensitivity_exit_time_dict.items(), key=lambda x: float(re.search(r"alpha([0-9.]+)", x[0]).group(1)))
    # x_alpha_exit = [float(re.search(r"alpha([0-9.]+)", k).group(1)) for k, _ in sorted_exit_items]
    # y_avg_exit = [v for _, v in sorted_exit_items]

    # plt.figure(figsize=(10, 5))
    # plt.scatter(x_alpha_exit, y_avg_exit, color="b")
    # plt.xlabel("Alpha Value", fontsize=18) 
    # plt.ylabel("Average Exit Time (s)", fontsize=18)
    # plt.xticks(fontsize=14)
    # plt.yticks(fontsize=14)
    # plt.xlim(left=0)
    # plt.ylim(bottom=0)
    # plt.title("Average Exit Time")
    # plt.grid(True)
    # plt.savefig(os.path.join(fig_path, "average_exit_time.png"))
    # plt.close()

    # ###################### Average Delivery Rate (Sorted) ######################################################
    # ########################### Plot avg delivery rate
    # sorted_delivery_rate_items = sorted(average_delivery_rate_dict.items(), key=lambda x: float(re.search(r"alpha([0-9.]+)", x[0]).group(1)))
    # x_alpha_delivery_rate = [float(re.search(r"alpha([0-9.]+)", k).group(1)) for k, _ in sorted_delivery_rate_items]
    # y_avg_delivery_rate = [v for _, v in sorted_delivery_rate_items]

    # plt.figure(figsize=(10, 5))
    # plt.scatter(x_alpha_delivery_rate, y_avg_delivery_rate, color="b")
    # plt.xlabel("Alpha Value", fontsize=18)
    # plt.ylabel("Average Delivery Rate (Mb/s)", fontsize=18)
    # plt.xticks(fontsize=14)
    # plt.yticks(fontsize=14)
    # plt.xlim(left=0)
    # plt.ylim(bottom=0)
    # plt.title("Average Delivery Rate")
    # plt.grid(True)
    # plt.savefig(os.path.join(fig_path, "average_delivery_rate_dict.png"))
    # plt.close()




########################################################################################################################################


# # import os
# # import re
# # import numpy as np
# # import pandas as pd
# # import matplotlib.pyplot as plt

# # # Paths
# # txt_folder = "/home/maryam/SEARCH/SEARCH_TEST_FRAMEWORK/test_framework/framework/alpha1/output_viasat_cubic2"
# # csv_folder = "/home/maryam/SEARCH/SEARCH_TEST_FRAMEWORK/test_framework/framework/alpha1/output_viasat_cubic2/corresponding_pcap"

# # # Constants
# # SERVER_IP = "130.215.28.249"  # Adjust this if your server IP is different
# # INTERVAL = 0.05  # Throughput calculation interval in seconds

# # # Function to extract data from text files and corresponding CSV files
# # def get_data(file_path, pcap_csv_path):
# #     with open(file_path, 'r') as f:
# #         lines = f.readlines()
    
# #     now_list = []
# #     exit_time = None
# #     first_loss_time = None
# #     passed_bins = []

# #     found_exit = False
# #     found_loss = False
    
# #     for i, line in enumerate(lines):
# #         # Capture now_us time
# #         match_now = re.search(r"now_us: (\d+)", line)
# #         if match_now:
# #             now_us = int(match_now.group(1)) * 1e-6  # Convert to seconds
# #             now_list.append(now_us)

# #         # Capture exit time
# #         match_exit = re.search(r"Exit Slow Start at (\d+)", line)
# #         if match_exit and not found_exit:
# #             exit_time = int(match_exit.group(1)) * 1e-6  # Convert to seconds
# #             found_exit = True

# #         # Capture first loss time (first occurrence)
# #         match_loss = re.search(r"loss happen: (\d+)", line)
# #         if match_loss:
# #             loss_val = int(match_loss.group(1))
# #             if loss_val == 1 and not found_loss:
# #                 # Get previous `now_us` for accurate loss timestamp
# #                 prev_line = lines[i - 1] if i > 0 else ""
# #                 match_time = re.search(r"now_us: (\d+)", prev_line)
# #                 if match_time:
# #                     first_loss_time = int(match_time.group(1)) * 1e-6  # Convert to seconds
# #                 found_loss = True

# #         # Capture passed bins count
# #         match_bins = re.search(r"passed_bins (\d+)", line)
# #         if match_bins:
# #             passed_bins.append((exit_time or first_loss_time, int(match_bins.group(1))))

# #     # Load CSV and compute throughput
# #     df = pd.read_csv(pcap_csv_path)

# #     # Normalize time (sync with first significant ack)
# #     first_ack_row = df[df['Ack number'] > 1000].iloc[0]
# #     time_first_ack = first_ack_row['Time']
# #     df = df[df['Time'] >= time_first_ack]
# #     df['Time'] -= time_first_ack

# #     # Filter valid packets for throughput calculation
# #     df_valid = df[(df['Source'] == SERVER_IP) & (df['retransmission'].isna())]

# #     # Initialize throughput calculation
# #     throughputs = []
# #     timestamps = []

# #     start_time = df_valid['Time'].iloc[0]
# #     end_time = start_time + INTERVAL

# #     # Compute throughput in fixed intervals
# #     while end_time <= df_valid['Time'].iloc[-1]:
# #         window_data = df_valid.loc[(df_valid['Time'] >= start_time) & (df_valid['Time'] < end_time)]
# #         if not window_data.empty:
# #             total_bytes = window_data['Length'].sum() * 8 * 1e-6  # Convert to Megabits
# #             throughput = total_bytes / INTERVAL  # Mbps
# #             throughputs.append(throughput)
# #             timestamps.append(end_time)

# #         # Move to next window
# #         start_time = end_time
# #         end_time = start_time + INTERVAL

# #     # Convert throughput list into DataFrame for safer access
# #     throughput_df = pd.DataFrame({"Time": timestamps, "Throughput": throughputs})

# #     median_throughput = np.median(throughputs) if throughputs else 0

# #     # Ensure `exit_throughput` is only computed if data exists
# #     if exit_time and not throughput_df.empty:
# #         exit_idx = (throughput_df["Time"] - exit_time / 1e6).abs().idxmin()
# #         exit_throughput = throughput_df.iloc[exit_idx]["Throughput"]
# #     else:
# #         exit_throughput = np.nan

# #     file_results.append((file_idx, exit_time, first_loss_time, median_throughput, exit_throughput))
# #     time_bin_data[file_idx] = (passed_bin_time, passed_bins)

# #     # Extract median throughput and throughput at exit time
# #     median_throughput = df["throughput"].median() if "throughput" in df.columns else np.nan
# #     throughput_at_exit = df.loc[df["Time"] >= (exit_time or first_loss_time), "throughput"].iloc[0] if not df.empty else 0

# #     return exit_time, first_loss_time, passed_bins, median_throughput, throughput_at_exit

# # # List of text files and corresponding CSV files
# # txt_files = sorted([f for f in os.listdir(txt_folder) if f.endswith(".txt")])
# # csv_files = sorted([f for f in os.listdir(csv_folder) if f.endswith(".csv")])

# # # Ensure file count matches
# # if len(txt_files) != len(csv_files):
# #     print("WARNING: Number of text and CSV files do not match!")

# # # Data collection
# # file_indices = []
# # exit_times = []
# # median_throughputs = []
# # exit_throughputs = []
# # time_passed_bins = []

# # for idx, (txt_file, csv_file) in enumerate(zip(txt_files, csv_files)):
# #     txt_path = os.path.join(txt_folder, txt_file)
# #     pcap_csv_path = os.path.join(csv_folder, csv_file)

# #     exit_time, first_loss_time, passed_bins, median_throughput, throughput_at_exit = get_data(txt_path, pcap_csv_path)
    
# #     if exit_time or first_loss_time:
# #         file_indices.append(idx + 1)
# #         exit_times.append(exit_time if exit_time else first_loss_time)
# #         median_throughputs.append(median_throughput)
# #         exit_throughputs.append(throughput_at_exit)
# #         time_passed_bins.append(passed_bins)

# # # Plot 1: Exit Time vs Throughput
# # fig, ax1 = plt.subplots()

# # ax1.set_xlabel("File Index")
# # ax1.set_ylabel("Exit Time (s)", color="tab:blue")
# # ax1.scatter(file_indices, exit_times, color="tab:blue", label="Exit Time")
# # ax1.tick_params(axis="y", labelcolor="tab:blue")

# # ax2 = ax1.twinx()
# # ax2.set_ylabel("Throughput (Mb/s)", color="tab:red")
# # ax2.scatter(file_indices, median_throughputs, color="red", label="Median Throughput", marker="o")
# # ax2.scatter(file_indices, exit_throughputs, color="green", label="Exit Throughput", marker="s")
# # ax2.tick_params(axis="y", labelcolor="tab:red")

# # fig.legend(loc="upper left", bbox_to_anchor=(0.1, 0.9))
# # plt.title("Exit Time vs. Throughput")
# # plt.show()

# # # Plot 2: Passed Bins Over Time
# # plt.figure()

# # for idx, bins in enumerate(time_passed_bins):
# #     if bins:
# #         times, bin_counts = zip(*bins)
# #         plt.plot(times, bin_counts, label=f"File {idx + 1}")

# # plt.xlabel("Time (s)")
# # plt.ylabel("Number of Passed Bins")
# # plt.title("Passed Bins Over Time")
# # plt.legend()
# # plt.show()

# # import os
# # import re
# # import numpy as np
# # import pandas as pd
# # import matplotlib.pyplot as plt

# # # Paths
# # txt_folder = "/home/maryam/SEARCH/SEARCH_TEST_FRAMEWORK/test_framework/framework/alpha1/output_viasat_cubic2"
# # csv_folder = "/home/maryam/SEARCH/SEARCH_TEST_FRAMEWORK/test_framework/framework/alpha1/output_viasat_cubic2/corresponding_pcap"

# # # Function to extract data from text files and corresponding CSV files
# # def get_data(file_path, pcap_csv_path):
# #     with open(file_path, 'r') as f:
# #         lines = f.readlines()
    
# #     now_list = []
# #     exit_time = None
# #     first_loss_time = None
# #     passed_bins = []

# #     found_exit = False
# #     found_loss = False

# #     # Read CSV file to extract throughput data
# #     df = pd.read_csv(pcap_csv_path)
    
# #     for i, line in enumerate(lines):
# #         # Capture now_us time
# #         match_now = re.search(r"now_us: (\d+)", line)
# #         if match_now:
# #             now_us = int(match_now.group(1)) * 1e-6  # Convert to seconds
# #             now_list.append(now_us)

# #         # Capture exit time
# #         match_exit = re.search(r"Exit Slow Start at (\d+)", line)
# #         if match_exit and not found_exit:
# #             exit_time = int(match_exit.group(1)) * 1e-6  # Convert to seconds
# #             found_exit = True

# #         # Capture first loss time (first occurrence)
# #         match_loss = re.search(r"loss happen: (\d+)", line)
# #         if match_loss:
# #             loss_val = int(match_loss.group(1))
# #             if loss_val == 1 and not found_loss:
# #                 # Get previous `now_us` for accurate loss timestamp
# #                 prev_line = lines[i - 1] if i > 0 else ""
# #                 match_time = re.search(r"now_us: (\d+)", prev_line)
# #                 if match_time:
# #                     first_loss_time = int(match_time.group(1)) * 1e-6  # Convert to seconds
# #                 found_loss = True

# #         # Capture passed bins count
# #         match_bins = re.search(r"passed_bins (\d+)", line)
# #         if match_bins:
# #             passed_bins.append((exit_time or first_loss_time, int(match_bins.group(1))))

# #     # Extract median throughput and throughput at exit time
# #     median_throughput = df["throughput"].median() if "throughput" in df.columns else np.nan
# #     throughput_at_exit = df.loc[df["Time"] >= (exit_time or first_loss_time), "throughput"].iloc[0] if not df.empty else 0

# #     return exit_time, first_loss_time, passed_bins, median_throughput, throughput_at_exit

# # # List of text files and corresponding CSV files
# # txt_files = sorted([f for f in os.listdir(txt_folder) if f.endswith(".txt")])
# # csv_files = sorted([f for f in os.listdir(csv_folder) if f.endswith(".csv")])

# # # Ensure file count matches
# # if len(txt_files) != len(csv_files):
# #     print("WARNING: Number of text and CSV files do not match!")

# # # Data collection
# # file_indices = []
# # exit_times = []
# # median_throughputs = []
# # exit_throughputs = []
# # time_passed_bins = []

# # for idx, (txt_file, csv_file) in enumerate(zip(txt_files, csv_files)):
# #     txt_path = os.path.join(txt_folder, txt_file)
# #     pcap_csv_path = os.path.join(csv_folder, csv_file)

# #     exit_time, first_loss_time, passed_bins, median_throughput, throughput_at_exit = get_data(txt_path, pcap_csv_path)
    
# #     if exit_time or first_loss_time:
# #         file_indices.append(idx + 1)
# #         exit_times.append(exit_time if exit_time else first_loss_time)
# #         median_throughputs.append(median_throughput)
# #         exit_throughputs.append(throughput_at_exit)
# #         time_passed_bins.append(passed_bins)

# # # Plot 1: Exit Time vs Throughput
# # fig, ax1 = plt.subplots()

# # ax1.set_xlabel("File Index")
# # ax1.set_ylabel("Exit Time (s)", color="tab:blue")
# # ax1.scatter(file_indices, exit_times, color="tab:blue", label="Exit Time")
# # ax1.tick_params(axis="y", labelcolor="tab:blue")

# # ax2 = ax1.twinx()
# # ax2.set_ylabel("Throughput (Mb/s)", color="tab:red")
# # ax2.scatter(file_indices, median_throughputs, color="red", label="Median Throughput", marker="o")
# # ax2.scatter(file_indices, exit_throughputs, color="green", label="Exit Throughput", marker="s")
# # ax2.tick_params(axis="y", labelcolor="tab:red")

# # fig.legend(loc="upper left", bbox_to_anchor=(0.1, 0.9))
# # plt.title("Exit Time vs. Throughput")
# # plt.show()

# # # Plot 2: Passed Bins Over Time
# # plt.figure()

# # for idx, bins in enumerate(time_passed_bins):
# #     if bins:
# #         times, bin_counts = zip(*bins)
# #         plt.plot(times, bin_counts, label=f"File {idx + 1}")

# # plt.xlabel("Time (s)")
# # plt.ylabel("Number of Passed Bins")
# # plt.title("Passed Bins Over Time")
# # plt.legend()
# # plt.show()

# import os
# import re
# import numpy as np
# import pandas as pd
# import matplotlib.pyplot as plt

# # Define paths
# txt_folder = "/home/maryam/SEARCH/SEARCH_TEST_FRAMEWORK/test_framework/framework/alpha1/output_4g_cubic"
# csv_folder = "/home/maryam/SEARCH/SEARCH_TEST_FRAMEWORK/test_framework/framework/alpha1/output_4g_cubic/pcap"
# save_folder = "/home/maryam/SEARCH/SEARCH_TEST_FRAMEWORK/test_framework/framework/alpha1/output_4g_cubic/plots"

# # Ensure save directory exists
# os.makedirs(save_folder, exist_ok=True)

# # Constants
# SERVER_IP = "130.215.28.249"  # Adjust based on your setup
# INTERVAL = 0.05  # Throughput calculation interval in seconds

# # Store results
# file_results = []
# time_bin_data = {}

# # List of text and CSV files
# txt_files = sorted([f for f in os.listdir(txt_folder) if f.endswith(".txt")])
# csv_files = sorted([f for f in os.listdir(csv_folder) if f.endswith(".csv")])

# # Process each text file
# for file_idx, (txt_file, csv_file) in enumerate(zip(txt_files, csv_files), start=1):
#     txt_path = os.path.join(txt_folder, txt_file)
#     pcap_csv_path = os.path.join(csv_folder, csv_file)

#     if not os.path.exists(pcap_csv_path):
#         print(f"Skipping {txt_file}: No corresponding CSV found.")
#         continue

#     print(f"Processing {txt_file}...")

#     # Initialize variables
#     exit_time, first_loss_time, passed_bins, passed_bin_time = None, None, [], []

#     # Read TXT File
#     with open(txt_path, "r") as f:
#         lines = f.readlines()

#     for i, line in enumerate(lines):
#         # Find "Exit Slow Start at <time>"
#         match_exit = re.search(r"Exit Slow Start at (\d+)", line)
#         if match_exit and exit_time is None:
#             exit_time = int(match_exit.group(1))

#         # Find first "loss happen: 1" after exit time
#         match_loss = re.search(r"loss happen: (\d+)", line)
#         if match_loss:
#             loss_happen = int(match_loss.group(1))
#             if loss_happen == 1 and first_loss_time is None:
#                 prev_line = lines[i - 4] if i > 0 else ""
#                 match_time = re.search(r"now_us: (\d+)", prev_line)
#                 if match_time:
#                     first_loss_time = int(match_time.group(1))

#         # Count passed bins before exit/loss
#         if "passed_bins" in line:
#             match_bins = re.search(r"passed_bins (\d+)", line)
#             if match_bins and (exit_time is not None or first_loss_time is not None):
#                 passed_bins.append(int(match_bins.group(1)))
#                 prev_time_match = re.search(r"now_us: (\d+)", lines[i - 1]) if i > 0 else None
#                 if prev_time_match:
#                     passed_bin_time.append(int(prev_time_match.group(1)) * 1e-6)

#     # Load CSV and compute throughput
#     df = pd.read_csv(pcap_csv_path)

#     # Ensure valid Ack Number exists before normalizing time
#     if not df[df["Ack number"] > 1000].empty:
#         first_ack_row = df[df["Ack number"] > 1000].iloc[0]
#         time_first_ack = first_ack_row["Time"]
#         df = df[df["Time"] >= time_first_ack]
#         df["Time"] -= time_first_ack
#     else:
#         print(f"Skipping {csv_file}: No valid Ack number found.")
#         continue

#     # Filter valid packets for throughput calculation
#     df_valid = df[(df["Source"] == SERVER_IP) & (df["retransmission"].isna())]

#     # Initialize throughput calculation
#     throughputs = []
#     timestamps = []

#     if not df_valid.empty:
#         start_time = df_valid["Time"].iloc[0]
#         end_time = start_time + INTERVAL

#         # Compute throughput in fixed intervals
#         while end_time <= df_valid["Time"].iloc[-1]:
#             window_data = df_valid.loc[(df_valid["Time"] >= start_time) & (df_valid["Time"] < end_time)]
#             if not window_data.empty:
#                 total_bytes = window_data["Length"].sum() * 8 * 1e-6  # Convert to Megabits
#                 throughput = total_bytes / INTERVAL  # Mbps
#                 throughputs.append(throughput)
#                 timestamps.append(end_time)

#             # Move to next window
#             start_time = end_time
#             end_time = start_time + INTERVAL

#     # Convert throughput list into DataFrame for safer access
#     throughput_df = pd.DataFrame({"Time": timestamps, "Throughput": throughputs})

#     median_throughput = np.median(throughputs) if throughputs else 0

#     # Ensure `exit_throughput` is only computed if data exists
#     exit_throughput = np.nan
#     if exit_time and not throughput_df.empty:
#         exit_idx = (throughput_df["Time"] - exit_time / 1e6).abs().idxmin()
#         exit_throughput = throughput_df.iloc[exit_idx]["Throughput"]

#     file_results.append((file_idx, exit_time, first_loss_time, median_throughput, exit_throughput))
#     time_bin_data[file_idx] = (passed_bin_time, passed_bins)

# # Check if data exists before plotting
# if not file_results:
#     print("No valid data extracted. Please check file formats.")
# else:
#     df_results = pd.DataFrame(file_results, columns=["File Index", "Exit Time", "First Loss Time", "Median Throughput", "Exit Throughput"])

#     # Plot 1: Exit Time vs Throughput
#     fig, ax1 = plt.subplots(figsize=(12, 6))
#     ax1.set_xlabel("File Index")
#     ax1.set_ylabel("Exit Time (s)", color="tab:blue")
#     ax1.scatter(df_results["File Index"], df_results["Exit Time"] / 1e6, color="tab:blue", label="Exit Time")
#     ax1.scatter(df_results["File Index"], df_results["First Loss Time"] / 1e6, color="tab:purple", label="First Loss Time")

#     ax2 = ax1.twinx()
#     ax2.set_ylabel("Throughput (Mb/s)", color="tab:red")
#     ax2.scatter(df_results["File Index"], df_results["Median Throughput"], color="red", marker="o", label="Median Throughput")
#     ax2.scatter(df_results["File Index"], df_results["Exit Throughput"], color="green", marker="s", label="Exit Throughput")

#     ax1.legend(loc="upper left")
#     ax2.legend(loc="upper right")
#     plt.title("Exit Time vs. Throughput")
#     plt.savefig(os.path.join(save_folder, "all.png"))

#     # Plot 2: Passed Bins Over Time
#     for file_idx, (times, bins) in time_bin_data.items():
#         if times and bins:
#             plt.figure(figsize=(10, 5))
#             plt.plot(times, bins, label=f"File {file_idx}", marker="o", linestyle="-")

#             # Add exit time marker
#             exit_time = df_results.loc[df_results["File Index"] == file_idx, "Exit Time"].values[0]
#             if not np.isnan(exit_time):
#                 plt.axvline(exit_time / 1e6, color="black", linestyle="dashed", label=f"Exit Time {file_idx}")

#             plt.xlabel("Time (s)")
#             plt.ylabel("Number of Passed Bins")
#             plt.title(f"Passed Bins Over Time - File {file_idx}")
#             plt.legend()

#             # Save figure
#             save_path = os.path.join(save_folder, f"passed_bins_file_{file_idx}.png")
#             plt.savefig(save_path)
#             plt.close()
